---
title: "Ejercicios EDA"
author: "Lidia Sánchez Mérida"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Hip dataset

Este dataset está compuesto por diversa información relativa a un tipo de estrellas denominado *Hipparcos*. Para más detalles: https://astrostatistics.psu.edu/datasets/HIP_star.html

```{r}
# Descargamos el dataset obteniendo la cabecera y normalizando el tamaño de las filas
#hip<-read.table("https://astrostatistics.psu.edu/datasets/HIP_star.dat", header=T, fill=T)
hip<-read.table("HIP_star.dat", header=T, fill=T)
# Dimensión
dim(hip)
# Nombre de las columnas
colnames(hip)
# Tipos de datos
str(hip)
```
Como se puede visualizar en los resultados anteriores, este dataset dispone de **2.719 muestras y 9 variables**, siendo todas ellas numéricas.

A continuación se calculan las medidas centrales, como la **media, la mediana y la moda**, para cada una de las columnas. Para ello utilizamos la función `apply` con el objetivo de realizar estos cálculos para cada una de las columnas.

```{r}
apply(hip, 2, mean)
apply(hip, 2, median)
apply(hip, 2, mode)
```

En el siguiente `chunk` se calcula el **mínimo y máximo** para cada una de las columnas del dataset. Para ello, seguiremos utilizando la función `apply` en combinación con la función `range`, que proporciona sendos valores en el mismo resultado.

```{r}
apply(hip, 2, range)
```

Ahora calcularemos la **varianza, desviación estándar y la desviación absoluta de la mediana** para la variable `RA`. 

```{r}
var(hip$RA)
sd(hip$RA)
mad(hip$RA)
```
A continuación probamos la función definida en los ejercicios para intentar comprobar si se pueden calcular **la mediana y la desviación absoluta de la mediana**. Y tal y como se puede observar en el siguiente `chunk`, se pueden calcular sin ningún problema.

```{r}
f = function(x) c(median(x), mad(x))  
# Cálculo de la mediana y su desviación absoluta a partir de la 3ª columna
f(hip[,3])
```
En este siguiente `chunk` se realizan los mismos cálculos anteriores pero para la variable `RA` usando el operado `%>%` de la librería **`dplyr`**.

```{r message=FALSE, warning=FALSE}
library(dplyr)
c(hip$RA %>% median(), hip$RA %>% mad())
```
A continuación se propone averiguar cuál sería el resultado de ejecutar **`apply(hip,2,f)`**, siendo `f` la función definida anteriormente para calcular la mediana y su desviación absoluta.

```{r}
apply(hip, 2, f)
```
Como se puede observar en los resultados, el `chunk` anterior calcula la mediana y la desviación absoluta de la mediana para cada una de las columnas del dataset.

Ahora pasamos a calcular el **cuartil 0.10 y 0.50** para la columna `RA` del dataset. Para ello utilizaremos la función `quantile` a la que le especificaremos como vector de probabilidades el cuartil inicial, el final y el número de pasos que debe realizar. Para obtener solo el cuartil 0.1 y 0.5 basta con con situarlos como inicio y fin de la secuencia, estableciendo una suma de 0.4 para no calcular ninguno más.

```{r}
quantile(hip$RA, probs=seq(0.1, 0.5, 0.4))
```
En caso de querer calcular los **cuatro cuartiles** podemos utilizar la misma función, solo que en este caso especificaremos como primer valor el Q1 (0.25) y 0.25 como suma para calcular los restantes.

```{r}
quantile(hip$RA, probs=seq(0.25, 1.0, 0.25))
```
A continuación se propone averiguar si la función `summary` proporciona la **distancia intercuartil**. Para ello la aplicamos sobre la misma variable anterior y como podemos ver en el siguiente `chunk`, esta medida no se proporciona directamente. Sin embargo, se puede calcular mediante la **diferencia del Q3 y del Q1**, gracias a que se pueden obtener dichos valores de manera independiente de los resultados de la función `summary`.

```{r}
hip_summ<-summary(hip$RA)
hip_summ
# Cálculo de IQR a partir de Q3-Q1
as.numeric(hip_summ[5]) - as.numeric(hip_summ[2])
```
El objetivo del siguiente `chunk` es analizar los resultados de la función que se define así como su aplicación al *HIP* dataset. Como se puede apreciar en la salida, esta función indica con un valor `TRUE` si existen valores perdidos para cada una de las columnas, mientras que devuelve `FALSE` en caso de que no se hayan encontrado. Para este dataset solo la **columna `B.V` tiene valores perdidos**.

```{r}
hasNA = function(x) any(is.na(x)) 
apply(hip, 2, hasNA)   
```
Si intentamos obtener el valor mínimo de la columna anterior, que contiene valores perdidos, utilizando la función **`min` nos devolverá `NA` como respuesta**. Para obtener el verdadero mínimo de esta columna deberemos especificar en la función `na.rm=TRUE` para que los ignore durante el proceso.

```{r}
min(hip$B.V)
min(hip$B.V, na.rm = TRUE)
```
Otra forma de hacerlo sería utilizar la función **`na.omit`**, que elimina todas las muestras en las que existan valores perdidos. De este modo podemos calcular el mínimo de la variable `B.V` sin aportar ningún argumento adicional.

```{r}
hip_nan = na.omit(hip)
# Suma de valores perdidos del dataset = 0
sum(is.na(hip_nan))
# Dimensiones del nuevo dataset sin NA
dim(hip_nan)
min(hip_nan$B.V)
```
En el siguiente `chunk` se propone una tercera forma de realizar un cálculo matemático generalizado para todo el dataset sin tener que modificarlo y adaptándose a los valores perdidos de cada columna. En este caso, calcularemos la **media para todas las columnas**, ignorando los valores `NA` existentes. De este modo, la eliminación de valores perdidos solo afectará a las medias de las columnas que dispongan de `NA`, como es el caso de la variable `B.V`.

```{r}
# Medias con el conjunto original
apply(hip, 2, mean)

# Medias con el conjunto original ignorando los NA para cada columna
apply(hip, 2, function(x) mean(x, na.rm=TRUE))
```
A continuación se pretende mostrar un **diagrama de cajas** del dataset al completo para observar la variabilidad de los datos almacenados en cada una de las columnas. En el primer gráfico, podemos observar que solo la variable `HIP` es visible puesto que las cajas de las restantes apenas tienen longitud. Esto puede deberse a sus **diferentes escalas**. Por ello, escalamos y centramos los datos para representar de nuevo el diagrama de cajas.
```{r}
# Diagrama de cajas del conjunto original
var_colors<-c("red", "yellow", "orange", "green", "cyan", "blue", "pink", "purple", "gray")
boxplot(hip, notch=TRUE, col=var_colors)

# Escalamos y centramos los datos
scaled_hip<-scale(hip)
# Diagrama de cajas con los datos normalizados
boxplot(scaled_hip, notch=TRUE, col=var_colors)
```
En el segundo gráfico podemos observar que las cajas de todas las variables se caracterizan por una pequeña longitud, lo que demuestra que **no existe apenas dispersión en los datos**, sino que cada variable está concentrada en un rango de valores particular. Por otro lado, podemos apreciar que algunas variables disponen de **outliers moderados**, excepto `pmDE` y `e_Plx` cuyos número de *outliers* es mayor así como su lejanía con respecto a la concentración de valores. Según la descripción de este dataset, de este gráfico podemos concluir que **las coordenadas de la longitud (`RA`) no están relacionadas con las de la latitud (`DE`)**.

A continuación, se muestra una **gráfica de puntos entre las variables `RA` y `DE`**, mostrando en color rojo aquellos puntos cuyo `DE` sea mayor que 0. Como podemos observar, no existe ninguna relación visible entre sendas variables puesto que en el gráfico representado no se aprecia ninguna distribución conocida.

```{r}
library(ggplot2)
ggplot(hip, aes(x=RA, y=DE)) + geom_point(color=ifelse(hip$DE > 0,'red','gray'))
```
Ahora vamos a probar a realizar el mismo gráfico anterior pero entre las variables **`RA` y `pmRA`**. En este caso sí parece existir una **relación cuadrática** entre sendas columnas puesto que la mayoría de los puntos siguen la forma de esta función, exceptuando algunos *outliers*. De este gráfico podemos concluir que el movimiento de las estrellas por el cielo es decreciente en aquellas longitudes pertenecientes al intervalo [0, 200], mientras que es creciente en la otra mitad.

```{r}
library(ggplot2)
ggplot(hip, aes(x=RA, y=pmRA)) + geom_point(color="orange")
```
En lugar de representar un gráfico de puntos por cada par de variables, utilizaremos la función **`scatterplotMatrix`** para llevarlo a cabo con todas ellas. Como podemos apreciar en los resultados, la mayoría de las variables **parecen no seguir una distribución normal**. Mientras que por otro lado, podemos observar que existen pocas variables relacionadas entre sí en función de los gráficos de puntos. 

```{r warning=FALSE}
library(car)
scatterplotMatrix(hip)
```

Por último, vamos a filtrar el dataset según las condiciones descritas en el ejercicio para crear uno nuevo que solo contenga las **estrellas Hyadas**. Este dataset dispone de **88 muestras y 9 variables**. A continuación, utilizando este nuevo dataset representamos las variables **`Vmag y B.V`** en un diagrama de puntos.

```{r}
attach(hip)
hyades<-hip %>% filter(RA > 50 & RA < 100) %>% filter(DE > 0 & DE < 25) %>% filter(pmRA > 90 & pmRA < 130) %>% filter(pmDE > -60 & pmDE < -10) %>% filter(e_Plx < 5) %>% filter(Vmag > 4 | B.V < 0.2)
# Dimensiones del nuevo dataset
dim(hyades)
# Diagrama de puntos entre Vmag y B.V
ggplot(hyades, aes(x=Vmag, y=B.V)) + geom_point(color="purple")
```
Tal y como podemos apreciar, parece que existe una **relación lineal** entre ambas variables puesto que los puntos representados siguen una línea recta ascendente, excepto por algunos *outliers moderados*. Por tanto, según la descripción de este dataset y considerando que la luminosidad de las estrellas está invertida, parece que las estrellas que brillan menos tienen más color que las más brillantes.

# InsectSprays dataset

Este dataset está compuesto por **72 muestras y 2 columnas** que representan el número de insectos extraídos de diferentes zonas tratadas con algunos insecticidas. Si dibujamos su diagrama de cajas, podemos ver cómo los insecticidad **A, B y F** tienen una mayor longitud, lo que indica que existe una mayor variación en los datos. Esto nos puede indicar que **su eficacia ha sido muy diferente en diversas zonas**. Además, en todos ellos se han contabilizado un mayor número de insectos después de haber sido aplicados.

Por el contrario, las cajas de los insecticidas **C, D y E** son de menor tamaño, lo que indica que sus datos son menos dispersos. Por tanto, estos tres productos han demostrado una **eficacia similar en diferentes zonas** donde han sido aplicados. Asimismo, podemos observar que el conteo de insectos existentes tras aplicarlos han sido menores que los tres comparados anteriormente. Por lo que podemos intuir que estos tres insecticidas son los que mejor comportamiento han demostrado.

En particular, podemos observar dos **outliers en los insecticidas C y D**, lo cual nos parece indicar que existen dos zonas en las que han aparecido una mayor cantidad de insectos que en las restantes. Por otro lado, las **medianas de C y E están muy cercanas al Q1, mientras que la mediana de D está muy cercana a Q3**. Esto nos puede indicar que en los insecticidas C y E hay muy poca variación en el conteo de insectos de los dos primeros intervalos, mientras que en el insecticida D ocurre al contrario, hay una mayor variabilidad de conteos hasta el último intervalo.

En mi opinión, el insecticida C es el que dispone de un conteo de insectos menor, por lo que parece demostrar una mayor eficacia en diferentes zonas con la excepción de aquella en la que se ha producido un *outlier*. En segundo lugar, el insectivida D es el que parece ser el segundo más eficaz en función de su rango de valores, aunque contiene un *outlier* mucho más alejado, comparado con el anterior insecticida. Sin embargo, la eficacia del **insecticida E** ha sido un poco más variable en las distintas zonas experimentadas, pero no contiene *outliers*, por lo que parece que su **comportamiento es más robusto y estable** independientemente de la zona en la que se aplique.

```{r}
library(datasets)
insect_sprays<-InsectSprays
dim(insect_sprays)
var_colors<-c("red", "yellow", "green", "cyan", "pink", "gray")
boxplot(count~spray, insect_sprays, col=var_colors)
```

# Carseats dataset

En el primer apartado se propone estudiar el grado de asimetría para cada una de las variables del dataset `Carseats`. Sin embargo, algunas de ellas disponen de valores nominales y por tanto no se puede aplicar la función `skewness` del paquete `moments`. Por ello, las codificaremos como etiquetas numéricas mediante la función `as.numeric`.

```{r}
# Cargamos el dataset
library(ISLR)
carseats <- Carseats
str(carseats)
# Codificamos las variables categóricas a etiquetas numéricas
carseats$ShelveLoc <- as.numeric(carseats$ShelveLoc)
carseats$Urban <- as.numeric(carseats$Urban)
carseats$US <- as.numeric(carseats$US)
str(carseats)
# Aplicamos la función skewness a cada variable
library(moments)
carseats_skewness <- sapply(c(1:ncol(carseats)), function(x) skewness(carseats[,x]))
carseats_skewness
# Variables asimétricas por la izquierda (skewness negativo)
left_skewness <- colnames(carseats)[which(carseats_skewness < -0.5)]
left_skewness
# Variables asimétricas por la izquierda (skewness positivo)
right_skewness <- colnames(carseats)[which(carseats_skewness > 0.5)]
right_skewness
```

Como podemos observar en los resultados anteriores, en la **primera lista se encuentran las variables con un grado de asimetría menor que -0.50**, por lo que disponen de colas situadas en la parte izquierda, mientras que en la **segunda lista aparece la única variable con un valor de asimetría mayor de 0.50**, y por ello tiene una menor concentración de datos a la derecha.

A continuación se pretende encontrar qué variables no siguen una distribución normal. Para ello, considerando los resultados anteriores, se representa un **gráfico `QQ` de cada una de las variables anteriores** para estudiar sus cuantiles teóricos y reales.

```{r}
qqnorm(carseats$ShelveLoc, main="QQ-plot ShelveLoc")
qqline(carseats$ShelveLoc)
qqnorm(carseats$Urban, main="QQ-plot Urban")
qqline(carseats$Urban)
qqnorm(carseats$US, main="QQ-plot US")
qqline(carseats$US)
qqnorm(carseats$Advertising, main="QQ-plot Advertising")
qqline(carseats$Advertising)
```

Como podemos apreciar en las tres primeras gráficas, las variables de la primera lista con una asimetría negativa parecen seguir una **distribución bimodal**, puesto que sus cuantiles reales representan una función a trozos. Esto nos indica que pueden existir varios máximos locales pero ninguno global, por lo que se diferencian considerablemente de una distribución normal. En cambio, la variable `Advertising` con una asimetría a la derecha sí parece seguir una distribución normal puesto que sus cuantiles teóricos y reales no son tan diferentes como en los casos anteriores.

Finalmente, a continuación se muestra un gráfico con las **correlaciones entre todas las variables** del dataset. Tal y como se observa, la mayoría de las variables no están relacionadas entre sí puesto que el tono de los colores es muy suave, a excepción de tres casos:
1. `US` y `Advertising` tienen un coeficiente de correlación de 0.68 por lo que existe una relación positiva y moderadamente fuerte entre sendas variables.
2. `Price` y `CompPrice` tienen un coeficiente de correlación de 0.58 por lo que también se puede afirmar que están levemente relacionadas de forma positiva entre sí.
3. `Sales` y `Price` son las dos únicas variables que destacan por su leve correlación negativa con un coeficiente de -0.44.

```{r}
# Cálculo de las correlaciones entre todas las variables y su representación con un intervalo de colores en la matriz inferior
library(corrplot)
corrplot(cor(carseats), method='number', order='alphabet', type='lower')
```

