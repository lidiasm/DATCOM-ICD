---
title: "Trabajo de Introducción a la Ciencia de Datos"
author: "Lidia Sánchez Mérida"
output: pdf_document
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Librerías necesarias 
require(car)
require(tidyverse)
require(psych)
require(corrplot)
require(kknn)

library(car)
library(tidyverse)
library(psych)
library(corrplot)
library(kknn)

# Semilla para que los resultados que dependen de la aleatoriedad
# sean reproducibles.
set.seed(0)
```

# 1. Apartado de Regresión

El dataset con el que se procede a trabajar para este primer apartado de aplicación de técnicas de Regresión se denomina **`abalone`**. En primer lugar, buscamos información sobre sus datos para descubrir sus características principales y composición. Según la descripción que proporciona [Kaggle](https://www.kaggle.com/rodolfomendes/abalone-dataset), se trata de un conjunto de **medidas físicas realizadas sobre un tipo de molusco para determinar su edad**. Por otro lado, en la página de [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/abalone) se detallan los siguientes aspectos:

* Dispone de un total de **9 variables** de diferente naturaleza:

| Variable | Tipo | Descripción | Valores |
|---|---|---|---|
| Sex | Nominal | Género del ejemplar | M (masculino), F (femenino), I (infante) |
| Length | Real | Longitud de la concha  | Milímetros |
| Diameter |  Real |  Ancho de la concha |  Milímetros |
| Height |  Real |  Altura del ejemplar | Milímetros |
| Whole weight |  Real |  Peso total del ejemplar | Gramos  |
| Shucked weight |  Real |  Peso del ejemplar sin la concha | Gramos |
| Viscera weight |  Real |  Peso de las vísceras del ejemplar | Gramos |
| Shell weight | Real  |  Peso de la concha sin el ejemplar |  Gramos |
| Rings | Entero  | Edad del animal (+1.5)  |   |
  
* Se trata de un conjunto de datos asociado a un problema de clasificación.
* **No contiene valores perdidos**.

## 1.1. Análisis Exploratorio de Datos General

A continuación, procedemos a analizar las principales propiedades de este conjunto de datos. En primer lugar cargamos el dataset desde el fichero `abalone.dat`, especificamos el separador para distinguir los valores de cada columna y asignamos manualmente el nombre de las variables a partir de la líneas `@inputs` y `@outputs` del fichero. Como podemos apreciar en el siguiente *chunk*, este dataset cuenta con **4.177 registros y 9 variables**. En el caso de la columna que indica el **género podemos apreciar que ya está codificada en este fichero**, por lo que asumiremos que las etiquetas numéricas corresponden a los siguientes valores nominales: 1-masculino, 2-femenino y 3-infante.

```{r}
# Nombres de las variables según el fichero `abalone.dat`
abalone.colnames <- c("Sex", "Length", "Diameter", "Height", "Whole_weight", 
                  "Shucked_weight", "Viscera_weight", "Shell_weight", "Ring")
# Leemos el dataset del fichero `abalone.dat`
# Parámetros
# 1. Ruta hacia el fichero
# 2. Las columnas no se encuentran en la primera fila
# 3. Ignoramos las 13 primeras filas hata alcanzar los datos
# 4. Separador de columnas
# 5. Nombres de las columnas
abalone.df <- read.table("./abalone/abalone.dat", header=FALSE, skip=13, 
                         sep=",", col.names=abalone.colnames)
# Dimensión 
dim(abalone.df)
# Tipos de las variables
str(abalone.df)
```
Aplicamos la función `summary` para obtener un resumen con las principales medidas estadísticas de cada variable. Tal y como se puede observar, el primer aspecto interesante ocurre en la variable `Sex` puesto que según sus cuartiles, en el primer intervalo se encuentran mayoritariamente ejemplares masculinos (etiqueta 1), mientras que en el último intervalo destacan los denominados infantes (etiqueta 3). Por otro lado, podemos apreciar que el ejemplar intermedio es femenino puesto que la mediana es 2. Esta distribución puede indicarnos que **esta variable de clase está balanceada** y existe aproximadamente el mismo número de ejemplares para cada categoría.

En relación a las dimensiones de la concha, podemos observar que no existe demasiada diferencia entre los respectivos cuartiles de las variables `Length` y `Diameter`. Además, sus medias y medianas son valores más cercanos al dato máximo, por lo que podemos intuir que existe una **concentración de ejemplares con una concha de mayor envergadura**. Sin embargo, este hecho contrasta con los valores de la variable `Height`, en los cuales destacan los siguientes aspectos:

1. En primer lugar me resulta extraño que existan **ejemplares con una altura de 0 milímetros**, incluyendo tanto la concha como el animal en el interior. No se conoce cómo se han realizado las mediciones y por lo tanto si podrían ser datos erróneos, pero este tipo de valores parecen ser interesantes para futuros estudios de valores anómalos.

2. Por otro lado, la diferencia entre los cuartiles Q1 y Q3 es aún más reducida que en los casos anteriores, lo que significa que existe una **concentración de ejemplares de baja altura**. Si además observamos la media y la mediana, también son valores muy cercanos al mínimo.

```{r}
# Resumen del dataset
summary(abalone.df)
```
En el caso de las medidas relativas a los diferentes tipos de pesos, parece haber una mayor dispersión de valores puesto que existen mayores diferencias entre sus respectivos cuartiles. No obstante, también podemos observar que sus medias y medianas son valores más cercanos al límite inferior de sus respectivos rangos de valores. Esto nos indica que **la mayoría de ejemplares no son pesados**, lo cual puede contrastar con las elevadas dimensiones de las conchas que hemos observado anteriormente.

Finalmente, cabe destacar que en la variable independiente `Ring` tampoco existe una diferencia considerable entre sus cuartiles, y su media y mediana son valores cercanos al mínimo, por lo que parece que en el conjunto de datos hay una **mayor representación de ejemplares jóvenes**. 

### 1.1.1. Hipótesis Iniciales

En función del resumen estadístico anterior, procedemos a plantear un conjunto de hipótesis iniciales acerca de las muestras, los predictores y la variable independiente.

1. En primer lugar, parece que existe un **número similar de ejemplares para cada género**, por lo que las clases de la variable `Sex` estarían balanceadas. Creo que esta variable puede ayudar a determinar los ejemplares más jóvenes gracias a la categoría infante, puesto que es lógico pensar que los **ejemplares más jóvenes tendrán características físicas diferentes** a los adultos. Sin embargo, no tengo claro qué papel podría jugar para el resto de clases.

2. Existe una **concentración de ejemplares con conchas de gran tamaño** según los valores estadísticos de las variables `Length` y `Diameter`. Estos predictores pueden influir en la variable independiente puesto que es lógico que un **ejemplar aumente su envergadura conforme aumente su edad**. Asimismo, también pienso que ambas variables van a estar relacionadas ya que también es lógico pensar que **a mayor longitud de la concha, mayor diámetro**, por lo que deberemos analizar sus coeficientes de correlación y seleccionar aquella que maximice la variabilidad de información para construir los modelos. 

3. En la variable `Height` se observa una concentración de **ejemplares con alturas extremadamente bajas, incluso de 0 milímetros**. Por un lado, me parece extraño este último dato puesto que, independientemente de la edad, no me parece que tenga sentido un valor de 0 en una medida física. Por otro lado, también me extraña la cantidad de **ejemplares de poca altura, en comparación con los considerables tamaños de sus conchas**. A mi parecer, en el primer caso podemos encontrarnos con medidas erróneas, mientras que con respecto a la segunda observación, pienso que este tipo de especie puede estar caracterizada por tener un grosor muy fino. Sin embargo, no consigo visualizar la importancia que puede jugar esta variable en la predicción de la edad de esta especie.

4. Las diferentes medidas de peso disponen de un rango más amplio de valores, con una **concentración de ejemplares de poco peso**. Este hecho me resulta extraño puesto que, como se ha comentado anteriormente, el tamaño de sus conchas es considerablemente grande, por lo que **cabría esperar que sus pesos también fueran mayores**. Por otro lado, creo que pueden existir las siguientes relaciones entre este tipo de predictores.

    * Todos los pesos particulares pueden estar relacionados linealmente con la variable que representa el peso total `Whole_weight`, por lo que puede ser lógico pensar que conforme **aumenten los pesos específicos, también aumente el peso general**. Esta teoría se podrá confirmar visualizando sus coeficientes de correlación.
  
    * Las variables `Shucked_weight` y `Viscera_weight` pueden estar relacionadas linealmente, puesto que me parece lógico pensar que si **aumenta el peso de los órganos de un ejemplar, también aumenta su peso corporal**. De igual modo, esta hipótesis podrá comprobarse mediante un gráfico de correlaciones.
  
5. Finalmente, cabe destacar que parece haber una **mayor representación de ejemplares jóvenes**. Este hecho puede suponer un problema en caso de que las características físicas varíen en función de la edad, puesto que si existen pocos ejemplares de mayor edad, los modelos pueden no aprender suficientemente bien los rasgos característicos de cada generación como para identificar las futuras muestras.

## 1.2. Análisis Exploratorios Univariantes

El objetivo de esta sección consiste en realizar un estudio detallado de cada variable dependiendo de la naturaleza de sus valores. En el caso de las **variables nominales, se pretende analizar el balanceamiento de sus clases** a través de gráficos de barras que representen la frecuencia de cada categoría. Mientras que para **variables escalares se realizarán análisis estadísticos** sobre la varianza, desviación típica, grado de asimetría, tipo de curtosis y distribución.

### 1.2.1. Variables nominales

En el caso de este dataset, la **única variable nominal es `Sex`**. Por ello, en este primer gráfico de barras se pretende mostrar el número de ejemplares de cada uno de los géneros disponibles: masculino (1), femenino (2) e infante (3). Nuestra **hipótesis inicial afirma que las clases están balanceadas**, por lo que el número de muestras de cada clase no debería de variar demasiado. Y tal y como podemos observar, podemos **aceptar dicha hipótesis** puesto que existen 1528 muestras masculinas, 1307 femeninas y 1342 ejemplares infantes, es decir, las diferentes categorías disponen de una representación razonablemente equivalente.

```{r}
# Definimos los géneros disponibles
genders <- c("Masculino", "Femenino", "Infante")
# Parámetros:
# 1. Convertimos las etiquetas a factores para obtener la frecuencia por clase
# 2. Contorno negro a las barras y la leyenda
# 3. Fondo blanco
# 4. Establecemos el nombre y las etiquetas de la leyenda para cada género
# 5. Ocultamos la etiqueta del eje X y establecemos la etiqueta del eje Y
# 6. Frecuencia de cada clase sobre sus respectivas barras
ggplot(abalone.df, aes(x=factor(Sex), fill=factor(Sex))) + 
  geom_bar(color="black") + theme_minimal() + 
  scale_fill_discrete(name='Géneros', labels=genders) + 
  labs(x="", y="Frecuencias") + 
  geom_text(stat='count', aes(label=..count..), vjust=2)
```

### 1.2.2. Variables numéricas

A continuación se calculan las medidas estadísticas mencionadas anteriormente para las variables numéricas de este dataset. Para la varianza se utiliza la función `var` aplicada a cada uno de los predictores, mientras que para la desviación típica, grado de asimetría y curtosis se hace uso de la **función `describe` del paquete `psych`**. Finalmente, con el objetivo de mostrar los resultados organizadamente, se define un `dataframe` cuyas columnas representan los cálculos realizados y cuyas filas indican los predictores asociados.

```{r}
# Cálculo de la varianza para cada variable numérica
abalone.vars <- sapply(c(2:ncol(abalone.df)), function(x) var(abalone.df[,x]))
# Cálculo de la desviación típica, asimetría y curtosis
abalone.statistics <- data.frame(describe(abalone.df%>%select(-Sex)))
# Dataframe con todas las medidas estadísticas por variable numérica
abalone.statistics <- abalone.statistics %>% select(c('sd', 'skew', 'kurtosis'))
cbind(var=abalone.vars, abalone.statistics)
```

Como podemos observar en los resultados, los predictores disponen de una **varianza mínima** puesto que sus valores se encuentran muy cercanos a 0. Este hecho nos indica que existe **muy poca variabilidad** en sus datos. En combinación con esta medida estadística, podemos observar que la **desviación estándar también es bastante baja**, lo cual indica que los valores de los predictores se encuentran **muy cercanos a la media**. La única excepción es la variable independiente `Ring`, cuya varianza y desviación típica son más elevadas ya que existen ejemplares de diferentes edades.

En relación con la forma de la distribución, podemos apreciar que la mayoría de variables disponen de un **grado de asimetría superior a 0.5**. Este hecho nos indica que tanto los predictores como la variable independiente tienen desiguales concentraciones de valores en zonas específicas de sus intervalos. En particular, todas las **medidas relativas al peso se encuentran encoladas a la derecha** puesto que su grado de asimetría es positivo. Mientras que las **medidas relativas al tamaño de la concha, como `Length` y `Diameter`, están encoladas a la izquierda** por sus coeficientes negativos. Los casos de la **variable independiente y del predictor `Height`** destacan sobre los restantes puesto que sus coeficiente son mayores que 1, lo cual indica que están caracterizadas por una **fuerte asimetría**, especialmente la variable dependiente. 

Situaciones similares ocurren con los **coeficientes de curtosis**, que indican la forma de las colas asociadas a las distribuciones. La mayoría de estos valores se encuentran en el intervalo [-0.05, 0.6], lo cual afirma que muchos predictores disponen de concentraciones menores en algunos extremos, pero al tratarse de **valores cercanos a 0 podemos intuir que siguen una distribución similar a la Gaussiana**. Sin embargo, esto no es aplicable ni a la variable **`Height`, cuyo coeficiente es muy elevado, ni al término independiente `Ring` cuyo valor también se aleja bastante de 0**. En el primer caso, la distribución del predictor estará caracterizada por una considerable concentración de valores en un intervalo muy reducido, mostrando un gran pico, mientras que para la variable `Ring` dicha acumulación de datos no será tan masiva pero también seguirá una forma similar.

Finalizando este estudio, a continuación procedemos a comprobar si siguen una **distribución normal**. Para ello vamos a experimentar con dos metodologías diferentes, una gráfica y otra estadística. En el primer caso vamos a hacer uso de los **gráficos QQ únicamente sobre las variables `Height` y `Ring`**, puesto que son las más peculiares según los resultados anteriores. Sin embargo, para confirmar la normalidad de cada variable se aplicará un test estadístico específico para esta demostrar esta propiedad: el **test de Shapiro-Wilk’s**. La hipótesis nula que se establece es que la variable sigue una distribución normal, frente a la alternativa contraria. Por lo tanto, si el test rechaza, es decir, obtiene un p-valor menor que el umbral=0.05, podemos determinar estadísticamente que la variable no sigue una distribución normal.

```{r}
# Gráfico QQ para la variable dependiente `Height`
qqnorm(abalone.df$Height, main="Gráfico QQ de Height")
qqline(abalone.df$Height)
# Gráfico QQ para la variable independiente `Ring`
qqnorm(abalone.df$Ring, main="Gráfico QQ de Ring")
qqline(abalone.df$Ring)
```

Tal y como podemos apreciar en el primer gráfico QQ asociado a `Height`, los cuantiles teóricos y empíricos coinciden prácticamente al 100%, a excepción de dos casos. Como consecuencia, este predictor parece que **gráficamente puede seguir una distribución normal**. No obstante, el caso de la variable independiente es diferente, puesto que parece haber una menor coincidencia entre ambas medidas, lo cual puede significar que la variable **`Ring` no sigue una distribución normal**. 

Antes de aplicar el test de Shapiro-Wilk’s sobre cada variable, mi hipótesis es que todas las variables parecen seguir una distribución normal, a excepción del término independiente `Ring`. Como se realizó anteriormente, se presentan los resultados de los tests en un `dataframe` para vincular cada variable con su p-valor asociado. Tal y como podemos apreciar, **todos los tests han rechazado la hipótesis nula** puesto que en todos se obtienen p-valores menores que el umbral establecido en 0.05. Esto significa que **ninguna de las variables sigue una distribución normal**, lo cual aumenta la importancia de utilizar tests estadísticos en lugar de métodos gráficos que pueden inducirnos a errores.

```{r}
# Aplicamos el test de Shapiro-Wilk’s para cada variable extrayendo el p-valor
abalone.shapiros <- sapply(c(2:ncol(abalone.df)), 
                     function(x) shapiro.test(abalone.df[,x])$p.value)
# Definimos un dataframe con los resultados para cada variable
abalone.shapiros.df <- data.frame(Variables=colnames(abalone.df%>%select(-Sex)), 
                                  PValores=abalone.shapiros)
abalone.shapiros.df
```
## 1.3. Análisis Exploratorio Multivariante

En esta sección se pretende realizar diversos análisis considerando la gran mayoría de variables con el objetivo de obtener una idea general de este conjunto de datos.

### 1.3.1. Matriz de Puntos

En primer lugar vamos a representar una matriz de puntos con todas las variables disponibles. En la diagonal se ha elegido representar las **curvas de densidad** para graficar sus distribuciones. Aunque su aplicación sobre variables nominales puede no tener sentido, incluimos el predictor `Sex` para distinguir las cualidades físicas de cada categoría. Para ello, aplicamos un **filtro por género** con el que representar cada clase con un color y forma diferente. Así, enriquecemos el gráfico con más información y podremos comenzar a contrastar las hipótesis iniciales.

Si observamos las distribuciones de todas las variables, excepto `Sex`, podemos apreciar que las relativas a los ejemplares masculinos y femeninos son prácticamente idénticas, puesto que se superponen en muchas de las gráficas. Este hecho puede indicarnos que **no existen diferencias físicas entre el género masculino y femenino**. No ocurre lo mismo con el caso de los infantes, ya que sus valores son inferiores en cada una de las medidas con respecto a los otros dos géneros, lo cual ya se había planteado inicialmente.

Por otro lado, si observamos las curvas de densidad de las variables `Length` y `Diameter`, podemos confirmar la teoría de que la mayoría de los ejemplares disponen de unas **conchas muy voluminosas en contraposición con la altura**, puesto que los valores de `Height` están mayormente concentrados al comienzo del intervalo. Esto nos puede indicar que este tipo de molusco dispone de una gran amplitud aunque tiene un grosor fino. Como se ha comentado anteriormente, en el caso de las medidas relativas a los diferentes pesos podemos observar que existe una mayor variabilidad puesto que su rango de valores es más amplio. De igual modo, si observamos las curvas de densidad de la variable independiente `Ring` podemos apreciar que la **mayoría de los ejemplares son bastante jóvenes** ya que el intervalo donde se concentran más es [5, 15]. 

```{r}
# Parámetros
# 1. Representa solo la diagonal inferior porque la matriz es simétrica
# 2. Diferencia los ejemplares por género cambiando el color y su forma
# 2.1. Masculino: círculos azules.
# 2.2. Femenino: triángulos naranjas.
# 2.3. Infantes: cruces grises.
# 3. Representa curvas de densidad en la diagonal
scatterplotMatrix(abalone.df, diagonal=list(method="density"), 
                  upper.panel=NULL, col=c("blue", "orange", "gray"),
                  group=as.factor(abalone.df$Sex))
```

La segunda parte de este estudio consiste en analizar las posibles asociaciones entre pares de variables. Tal y como podemos observar, parece ser que **la longitud está relacionada linealmente con el diámetro** de la concha, puesto que su gráfica asociada prácticamente representa una recta estrictamente creciente. Por lo que nuestra teoría se confirma de que a mayor longitud de la concha, mayor diámetro. Una situación similar ocurre con el resto de variables, ya que en todas las gráficas relacionadas con la variable `Length` se puede visualizar una función creciente, siendo de menor intensidad en el caso de la variable `Height`, cuya representación se parece más a una raíz cuadrada. Todas estas conclusiones parecen ser aplicables también a la variable `Diameter` puesto que las gráficas con el resto de variables son muy similares. Continuando con los diferentes pesos también podemos observar que sus vínculos son montónicos, es decir, aumentan simultáneamente, por lo que podemos intuir que durante el desarrollo de los ejemplares sus diferentes **medidas de peso siguen una misma tendencia a la alza**. 

Un caso peculiar lo encontramos entre la variable `Height` y las variables con los diferentes pesos. Si observamos sus gráficas, podemos apreciar un crecimiento muy considerable y abrupto al comienzo del intervalo, es decir, existe una concentración de **ejemplares con una elevada altura y bajos pesos**. Sin embargo, a diferencia del resto de gráficas, en estos casos podemos observar que existen varias muestras que no siguen esta tendencia, ya que sus alturas disminuyen conforme aumentan sus pesos. Esto puede indicar que los ejemplares de mayor edad pierden cualidades físicas, como ocurre en la mayoría de las otras especies animales.

Finalmente, si observamos la variable independiente `Ring` podemos apreciar que es creciente con respecto a la longitud y diámetro, por lo que parece que los ejemplares de **mayor edad, tienen una concha de mayor longitud y diámetro**. Con respecto a los pesos hay una mayor variabilidad, aunque en el caso del peso asociado a la concha sí se aprecia un aumento de sus valores conforme mayor es la edad. Sin embargo, en las restantes variables podemos observar que existen diferencias entre los distintos géneros. Mientras que la tendencia de los infantes en todas las variables de pesos se mantiene creciente, existen ejemplares masculinos y femeninos cuyos tamaños se estabilizan e incluso se reducen conforme aumenta la edad.

### 1.3.2. Correlaciones

En esta sección se pretende estudiar los coeficientes de correlación de todas las combinaciones de variables. El objetivo es identificar aquellos vínculos relevantes que puedan permitirnos seleccionar los predictores más prometedores para los futuros modelos. Tal y como se aprecia en el siguiente gráfico, existen una gran cantidad de variables correladas.

* Como había comentado anteriormente, se confirma la teoría de que la **variable `Length` y `Diamenter` están fuertemente correladas** puesto que ambas medidas son relativas al tamaño de la concha. Esta asociación nos permite seleccionar a una de las dos variables para construir los modelos. Del mismo, sendos predictores se encuentran también **fuertemente correladas con las diferentes medidas de peso**. Por lo tanto, parece que al ser medidas relativas al físico de los ejemplares, comparten cierto grado de información.

* Por otro lado, parece que la **contribución de la variable `Sex` al resto de variables no es demasiado relevante**, ya que sus coeficientes de correlación están por debajo de 0.5. Una razón explicativa podría ser que este predictor solo ayuda a identificar a ejemplares muy jóvenes, por lo que no cubre a la mayoría de muestras.

```{r}
# Representa los coeficientes de correlación para todas las variables
# Parámetros
# 1. Calcula los coeficientes de correlación
# 2. Representa los coeficientes con números
# 3. Orden alfabético de las variables
# 4. Representa solo la matriz diagonal inferior
# 5. Tamaño de las etiquetas y de los coeficientes de correlación
corrplot(cor(abalone.df), method='number', order='alphabet',
         type='lower', tl.cex=0.7, number.cex=0.7)
```

En función de las correlaciones resultantes entre los predictores y la variable independiente, podemos observar que la mayoría de sus coeficientes demuestran una relación moderada, a excepción de `Shell_weight`. Esta variable es la que presenta un valor más alto de 0.63 por lo que parece que es el predictor **más relacionado con el término independiente**. Como consecuencia, se postula como uno de los predictores que más probabilidad tiene de ayudar en la construcción de los modelos.

### 1.3.3. Diagramas de cajas

Finalizando el estudio multivariante, vamos a representar un diagrama de cajas con todas las variables numéricas para graficar sus dispersiones de datos y comprobar las hipótesis anteriores. Tal y como podemos apreciar, la mayoría de predictores tienen una longitud mínima, lo que significa que sus datos disponen de una **muy baja variabilidad**. Asimismo, también parece que sus **intervalos de valores son muy similares**, por lo que no existen variables con escalas predominantes que puedan afectar a los modelos. Otra conclusión que se puede extraer es que todos los predictores disponen de **outliers moderados**. Se trata de puntos exteriores muy cercanos a las cajas, por lo que parece que no son valores extremos. 

Sin embargo, en el caso de la variable independiente `Ring` podemos observar que su **dispersión es mayor**, puesto que su correspondiente caja tiene una mayor longitud. Este hecho indica que dispone de una mayor variabilidad en sus datos, lo que confirma nuestra teoría inicial. Igualmente, también se confirma la hipótesis de que existe una **concentración de ejemplares jóvenes** de una edad entre 5 y 15, y puede ser la razón por la que aparece un mayor número de outliers en las otras franjas de edad. Observándolos, podemos contar que el segundo grupo más representado es el que tiene una edad a partir de 15, y los que menos aparecen son aquellos que se encuentran en el intervalo [0, 5].

```{r}
# Definimos un color por cada variable
var_colors<-c("red", "yellow", "orange", "green", "cyan", "blue", "pink", "purple")
# Diagrama de cajas para cada variable
# Parámetros
# 1. Conjunto de datos a representar
# 2. Representa la asimetría de los datos
# 3. Colores para cada caja
boxplot(abalone.df %>% select(-Sex), notch=TRUE, col=var_colors)
```

## 1.4. Modelos de Regresión 

En este apartado se pretende construir diferentes modelos aplicando diversas técnicas de regresión para intentar maximizar el número de aciertos en la tarea de predecir la variable `Ring` de este dataset.

### 1.4.1. Regresión Lineal Simple

Comenzamos utilizando la Regresión Lineal Simple entrenando un modelo distinto para cada uno de los cinco predictores más prometedores. Según los estudios anteriores, la variable **`Shell_weight` es la que dispone de un coeficiente de correlación mayor con respecto `Ring`**. Por lo tanto, el primer modelo estará compuesto por este predictor en solitario. 

```{r}
# Primer modelo: Ring~Shell_weight
abalone.ls1 <- lm(Ring~Shell_weight, data=abalone.df)
summary(abalone.ls1)
```
Como podemos apreciar en los resultados, el p-valor del predictor es menor que el umbral 0.05 y dispone de un alto nivel de significación, por lo que **la variable `Shell_weight` resulta útil para la predicción de `Ring`**. Sin embargo, el valor de R² ajustado indica que este modelo solo es capaz de explicar un 39% aproximadamente de los datos, mientras que tiene asociado un RSE de 2.51. Como consecuencia, parece que será necesario añadir más variables con el fin de mejorar su capacidad de generalización.

A continuación, procedemos a entrenar un **segundo modelo con la variable `Diameter`**, puesto que ha sido otro de los predictores que están más cláramente asociados con la variable independiente. Para ello, reproducimos el mismo proceso anterior.

```{r}
# Segundo modelo: Ring~Diameter
abalone.ls2 <- lm(Ring~Diameter, data=abalone.df)
summary(abalone.ls2)
```
Como podemos observar, los resultados son muy similares a los del modelo anterior, puesto que el nivel de significación estadística del predictor `Diameter` es muy alto. Sin embargo, en este modelo el **RSE aumenta ligeramente** mientras que el valor de **R² ajustado disminuye** hasta un 33%. Este hecho nos puede indicar que la variable `Diameter` también proporciona información útil para predecir la variable independiente `Ring`, aunque en menor medida que el predictor `Shell_weight`.

Para el tercer modelo vamos a utilizar la variable `Length`, que como hemos observado anteriormente, está fuertemente correlada con `Diameter` y con la variable independiente. El objetivo consiste en entrenar un modelo diferente para cada predictor para comparar su resultados y conocer **cuál es el que proporciona más información** con la que predecir la edad de los ejemplares.

```{r}
# Tercer modelo: Ring~Length
abalone.ls3 <- lm(Ring~Length, data=abalone.df)
summary(abalone.ls3)
```
En comparación con el modelo anterior, podemos observar que este tercer modelo entrenado con `Length` dispone de un **valor mayor de RSE y de R² ajustado**. Por tanto, este predictor aporta menor cantidad de información para la predicción de la variable independiente. Así, en caso de tener que seleccionar una de las dos, parece ser que la variable `Diameter` es la más prometedora para determinar la edad de las muestras de este dataset.

El cuarto modelo se encuentra compuesto de la variable `Height`, una de las más peculiares de este dataset, según hemos podido comprobar. Como se ha observado en los diferentes análisis, este predictor dispone de una distribución más extraña, al compararla con el resto, y aporta información general al tamaño del ejemplar considerando tanto el animal como su concha.

```{r}
# Cuarto modelo: Ring~Height
abalone.ls4 <- lm(Ring~Height, data=abalone.df)
summary(abalone.ls4)
```
Tal y como se aprecia en el resumen del modelo, el predictor `Height` no mejora ni la explicabilidad de la variable independiente ni el RSE, incluso se puede observar un **ligero empeoramiento de las medidas** de calidad. Este hecho nos indica que aporta menos información útil para la predicción de la variable `Ring` que en los casos anteriores.

Para el quinto y último modelo el predictor participante es `Whole_weight`, una medida de peso que involucra al cuerpo y a la concha del ejemplar. Es otra de las variables que disponen de una correlación moderada con respecto a la variable independiente, aunque en menor medida que los predictores anteriores, por lo que no esperamos que produzca una considerable mejoría.

```{r}
# Cuarto modelo: Ring~Whole_weight
abalone.ls5 <- lm(Ring~Whole_weight, data=abalone.df)
summary(abalone.ls5)
```
Tal y como se comentaba anteriormente, **esta variable no está tan fuertemente asociada** a la variable independiente y por ello su RSE aumenta, mientras que el valor de R² ajustado disminuye hasta situarse por debajo del 30%. Por lo tanto, este predictor en solitario tampoco es suficiente como para explicar la mayor parte de la variabilidad de los datos.

Finalmente, las conclusiones que podemos extraer de los análisis de modelos entrenados con Regresión Lineal Simple se detallan a continuación:

1. El modelo con un **menor RSE y un mayor valor de R² ajustado** ha sido el primero en el que intervenía únicamente la **variable `Shell_weight`**, la cual estaba **más fuertemente correlada con la variable independiente** `Ring`. Por lo tanto, se trata de un predictor que aporta suficiente información como para alcanzar por sí solo casi un 40% de la explicabilidad de los datos.

2. Pese a la extraña distribución y a su menor grado de correlación con la variable independiente, el **predictor `Height` es el que ha proporcionado el segundo mejor modelo** con un 31% de explicabilidad y un RSE de 2.67. Este hecho nos indica que la altura puede identificar a un tercio de los ejemplares de manera independiente. Con una alta probabilidad, estas muestras pertenecerán a la categoría de infantes, puesto que parece lógico pensar que estarán caracterizados por una menor altura que los adultos.

3. Finalmente, en los modelos con múltiples predictores deberemos de **eliminar la variable `Length`** debido a su alta correlación con el predictor `Diameter`. Si bien proporcionan el mismo tipo de información, esta última variable ha conseguido obtener un modelo con una mayor explicabilidad y menor error, por lo que su información resulta más útil para la predicción de la edad de esta especie.

### 1.4.2. Regresión Lineal Múltiple

El objetivo de este apartado consiste en realizar diferentes combinaciones de predictores para entrenar modelos con Regresión Lineal Múltiple y mejorar los resultados del mejor modelo obtenido con Regresión Lineal Simple. Para ello aplicaremos la **técnica backward** en el que comenzamos con un primer modelo en el que se incluyen todos los predictores, para después eliminar aquellos que no sean relevantes. 

```{r}
# Primer modelo: todos los predictores
abalone.lm1 <- lm(Ring~., data=abalone.df)
summary(abalone.lm1)
```

En el resumen estadístico podemos observar que, considerando todos los predictores, hemos conseguido mejorar la explicabilidad de los datos aumentando el valor de R² ajustado hasta un 53%, además de haber reducido el RSE. Sin embargo, podemos apreciar que el **predictor `Length` tiene asociado un p-valor muy superior** al umbral de 0.05, lo que nos indica que puede ser eliminado sin influir en el modelo. Este hecho ya se había previsto anteriormente si recordamos la fuerte correlación existente entre `Diameter` y `Length`. Por lo tanto, eliminamos esta variable y procedemos a entrenar un segundo modelo.

```{r}
# Segundo modelo: todos los predictores menos Length
abalone.lm2 <- lm(Ring~.-Length, data=abalone.df)
summary(abalone.lm2)
```
Tal y como se puede observar, este segundo modelo dispone de la **misma calidad** que el anterior solo que es **más sencillo** puesto que contiene un predictor menos. Observando de nuevo las variables, podemos apreciar que todas tienen una alta significación estadística, pero algunas disponen de p-valores más altos. Como el modelo todavía contiene muchos de los predictores iniciales, vamos a tratar de simplificarlo hasta conseguir un error y porcentaje de explicabilidad aceptable. En el tercer modelo vamos a eliminar la variable `Height` puesto que es la que mayor p-valor tiene asociado.

```{r}
# Tercer modelo: todos los predictores menos Length y Height
abalone.lm3 <- lm(Ring~.-Length-Height, data=abalone.df)
summary(abalone.lm3)
```
En este tercer modelo se ha reducido ligeramente el valor de R² ajustado y ha aumentado muy levemente el RSE. Sin embargo, al eliminar el predictor `Height` podemos observar cómo los **p-valores de algunas variables han aumentado**, en particular, las dos últimas: `Viscera_weight` y `Shell_weight`. Esto nos indica que la variable `Height` comparte cierta información común con estos dos predictores y al eliminarla, **se revalora su importancia** en el modelo. No obstante, el primero de ellos sigue distinguiéndose del resto por tener un p-valor más bajo. Vamos a entrenar un cuarto modelo eliminándola para comprobar cuál es el comportamiento.

```{r}
# Cuarto modelo: todos los predictores menos Length, Height y Viscera_weight
abalone.lm4 <- lm(Ring~.-Length -Height -Viscera_weight, data=abalone.df)
summary(abalone.lm4)
```
Como podemos apreciar, mientras que el RSE ha aumentado muy ligeramente el porcentaje de explicabilidad también se ha elevado un poco más. Esto nos indica que la información proporcionada por **la variable eliminada no es lo suficientemente importante** como para mantenerla en el modelo. Así, hemos conseguido un modelo más simplificado y con unas métricas de calidad razonablemente aceptables. 

A continuación procedemos a añadir **términos no lineales** para intentar ajustar más el modelo a los datos y mejorar su capacidad de generalización. En el quinto modelo se añade una **interacción entre las varaibles `Shell_weight` y `Diameter`**, con las que se han obtenido dos de los mejores modelos utilizando Regresión Lineal Simple. 

```{r}
# Quinto modelo: interacción entre Shell_weight y Diameter
abalone.lm5 <- lm(Ring~.-Length -Height -Viscera_weight
                  + Shell_weight*Diameter, data=abalone.df)
summary(abalone.lm5)
```
Como podemos apreciar en el resumen estadístico, la interacción entre sendas variables provoca la mejora tanto del porcentaje de explicabilidad, que aumenta hasta un 53.9%, mientras que por otro lado produce un ligero decremento del RSE. Esto nos indica que la **combinación de las variables `Shell_weight` y `Diameter`** permite un mayor ajuste a los datos, puesto que **potencia la información útil** para predecir la variable `Ring`. Sin embargo, podemos observar que de forma paralela también **reduce la significación estadística de la variable `Sex`**. Una posible explicación puede asociarse a que el tamaño y diámetro de la concha también pueden ser características diferenciadoras para los mismos ejemplares a los que la variable `Sex` ayuda a identificar: los infantes. Resulta lógico pensar que los animales más jóvenes de esta especie poseen conchas de menor tamaño. De hecho, si **eliminamos este predictor** podemos observar cómo en el sexto modelo las **medidas de calidad no se ven demasiado afectadas**, y de esta forma, conseguimos reducir la complejidad.

```{r}
# Sexto modelo: eliminando la variable Sex
abalone.lm6 <- lm(Ring~.-Length -Height -Viscera_weight -Sex
                  + Shell_weight*Diameter, data=abalone.df)
summary(abalone.lm6)
```
Realizando varios experimentos que no se incluyen en esta memoria para no extenderla demasiado, he podido comprobar que añadiendo términos cuadráticos también se consigue cierta mejora en la calidad del modelo, a costa de añadir complejidad. Un ejemplo representativo es el que se muestra en el séptimo modelo, en el que se añaden los **términos cuadráticos de dos medidas del peso: `Shucked_weight` y `Whole_weight`**. Como se puede apreciar, el porcentaje de R² ajustado aumenta a un 55%, mientras que el RSE disminuye cuatro centésimas. Se trata de otra forma con la que poder potenciar la información proporcionada individualmente por estos dos predictores.

```{r}
# Séptimo modelo: términos cuadráticos de Shucked_weight y Whole_weight
abalone.lm7 <- lm(Ring~.-Length -Viscera_weight -Sex + 
                    Shell_weight*Diameter +  I(Shucked_weight^2) + 
                    I(Whole_weight^2), data=abalone.df)
summary(abalone.lm7)
```
En comparación con el modelo obtenido mediante Regresión Lineal Simple, podemos observar que ha **aumentado la explicabilidad en más de un 16%, mientras que se reduce el error en 0.36 décimas**. Como principal desventaja se encuentra el aumento de la complejidad del modelo, puesto que en el múltiple intervienen cinco predictores, una interacción y dos términos cuadráticos, mientras que en el modelo simple solo dispone de una única variable dependiente.

### 1.4.3. KNN

En esta sección se pretende entrenar varios modelos utilizando el **algoritmo KNN en combinación con Validación Cruzada**. Para ello, utilizaremos las cinco particiones de entrenamiento y validación en cada una de las iteraciones. El objetivo consiste en analizar si el mejor modelo entrenado con estas técnicas dispone de una mejor capacidad predictora que el mejor modelo entrenado con Regresión Lineal Múltiple. Para ello vamos a utilizar la última formulación que ha producido los mejores resultados. 

```{r}
# Función para aplicar validación cruzada para entrenar y validar 5
# modelos con el algoritmo KNN y la formulación del mejor modelo de
# Regresión Lineal Múltiple.
# Fórmula original: Ring~.-Length -Viscera_weight -Sex 
# + Shell_weight*Diameter +        I(Shucked_weight^2
# Fórmula para la función: -X2 -X7 -X1 + X8*X3 +  I(X6^2) + I(X5^2)
nombre <- "abalone"
run_knn_fold <- function(i, x, tt = "test", formula = "all") {
  file <- paste(x, "-5-", i, "tra.dat", sep="")
  x_tra <- read.csv(paste("./abalone/", file, sep=""), 
                    comment.char="@", header=FALSE)
  file <- paste(x, "-5-", i, "tst.dat", sep="")
  x_tst <- read.csv(paste("./abalone/", file, sep=""), 
                    comment.char="@", header=FALSE)
  In <- length(names(x_tra)) - 1
  names(x_tra)[1:In] <- paste ("X", 1:In, sep="")
  names(x_tra)[In+1] <- "Y"
  names(x_tst)[1:In] <- paste ("X", 1:In, sep="")
  names(x_tst)[In+1] <- "Y"
  if (tt == "train") {
    test <- x_tra
  }
  else {
    test <- x_tst
  }
  # Entrena un modelo con todas las variables
  if (formula == "all") {
    fitMulti <- kknn(Y~., x_tra, test)
  }
  # Entrena un modelo con la fórmula del mejor modelo de Regresión
  # Lineal Múltiple
  else {
    fitMulti <- kknn(Y~.-X2 -X7 -X1 + X8*X3 +  I(X6^2) + I(X5^2), x_tra, test)
  }
  yprime <- fitMulti$fitted.values
  sum(abs(test$Y-yprime)^2)/length(yprime) ##MSE
}
# Media del error para entrenamiento y validación de los modelos de KNN
# con la fórmula del mejor modelo con Regresión Lineal Múltiple
knnMSEtrain <- mean(sapply(1:5, run_knn_fold, nombre, "train", "RLM"))
knnMSEtrain
knnMSEtest <- mean(sapply(1:5, run_knn_fold, nombre, "test", "RLM"))
knnMSEtest
```
Como podemos observar, el **error obtenido en el conjunto de entrenamiento es mayor que el del mejor modelo con Regresión Lineal Múltiple**. Esto demuestra que no siempre una misma formulación proporciona buenos resultados para algoritmos diferentes, como ha sido el caso de esta comparación entre KNN y Regresión Lineal Múltiple. 

## 1.5. Comparación de algoritmos

Para finalizar este apartado de regresión, se realiza una comparación entre el comportamiento de diversos algoritmos aplicados sobre diferentes datasets. En primer lugar, aplicamos la Validación Cruzada para entrenar **cinco modelos con Regresión Lineal Múltiple y otros cinco con el algoritmo KNN**, tal y como se realizó en la sección anterior, solo que en este caso **se consideran todos los predictores**. 

```{r}
# Función para aplicar validación cruzada para entrenar y validar 5
# modelos con Regresión Lineal Múltiple
nombre <- "abalone"
run_lm_fold <- function(i, x, tt = "test", formula = "all") {
  file <- paste(x, "-5-", i, "tra.dat", sep="")
  x_tra <- read.csv(paste("./abalone/", file, sep=""), 
                    comment.char="@", header=FALSE)
  file <- paste(x, "-5-", i, "tst.dat", sep="")
  x_tst <- read.csv(paste("./abalone/", file, sep=""), 
                    comment.char="@", header=FALSE)
  In <- length(names(x_tra)) - 1
  names(x_tra)[1:In] <- paste ("X", 1:In, sep="")
  names(x_tra)[In+1] <- "Y"
  names(x_tst)[1:In] <- paste ("X", 1:In, sep="")
  names(x_tst)[In+1] <- "Y"
  if (tt == "train") {
    test <- x_tra
  }
  else {
    test <- x_tst
  }
  fitMulti=lm(Y~.,x_tra)
  yprime=predict(fitMulti,test)
  sum(abs(test$Y-yprime)^2)/length(yprime) ##MSE
}
# Media del error de entrenamiento y validación para los modelos de 
# Regresión Lineal Múltiple considerando todas las variables
lmMSEtrain<-mean(sapply(1:5,run_lm_fold,nombre,"train"))
lmMSEtrain
lmMSEtest<-mean(sapply(1:5,run_lm_fold,nombre,"test"))
lmMSEtest
# Media del error de entrenamiento y validación para los modelos con 
# KNN considerando todas las variables
knnMSEtrain <- mean(sapply(1:5, run_knn_fold, nombre, "train"))
knnMSEtrain
knnMSEtest <- mean(sapply(1:5, run_knn_fold, nombre, "test"))
knnMSEtest
```
Una vez disponemos de los errores medios de entrenamiento y validación para cada técnica, los sustituimos en los ficheros `regr_train_alumnos.csv` y `regr_test_alumnos.csv`, en la línea donde se encuentran los resultados para el dataset `abalone`. A continuación, cargamos la información de sendos archivos para comenzar a realizar las comparaciones estadísticas. En primer lugar, enfrentamos los algoritmos **Regresión Lineal Múltiple y KNN utilizando el test de Wilcoxon** con el objetivo de conocer si existen diferencias significativas entre sendas técnicas. La hipótesis nula afirma que ambos algoritmos se comportan de forma semejante, mientras que la alternativa determina que no son iguales. 

```{r}
# Leemos los resultados sobre el conjunto de entrenamiento
resultados <- read.csv("regr_train_alumnos.csv")
tablatra <- cbind(resultados[,2:dim(resultados)[2]])
colnames(tablatra) <- names(resultados)[2:dim(resultados)[2]]
rownames(tablatra) <- resultados[,1]
# Leemos los resultados sobre el conjunto de test
resultados <- read.csv("regr_test_alumnos.csv")
tablatst <- cbind(resultados[,2:dim(resultados)[2]])
colnames(tablatst) <- names(resultados)[2:dim(resultados)[2]]
rownames(tablatst) <- resultados[,1]

# Normalizamos la tabla de resultados de tests para Wilcoxon
# ""+ 0.1 porque wilcox R falla para valores == 0 en la tabla""
difs <- (tablatst[,1] - tablatst[,2]) / tablatst[,1]
wilc_1_2 <- cbind(ifelse (difs<0, abs(difs)+0.1, 0+0.1), 
                  ifelse (difs>0, 	abs(difs)+0.1, 0+0.1))
colnames(wilc_1_2) <- c(colnames(tablatst)[1], colnames(tablatst)[2])
# Aplicamos el test de Wilcoxon sobre Regresión Lineal Múltiple y KNN
LMvsKNNtst <- wilcox.test(wilc_1_2[,1], wilc_1_2[,2], 
                          alternative = "two.sided", paired=TRUE)
Rmas <- LMvsKNNtst$statistic
pvalue <- LMvsKNNtst$p.value
LMvsKNNtst <- wilcox.test(wilc_1_2[,2], wilc_1_2[,1], 
                          alternative = "two.sided", paired=TRUE)
Rmenos <- LMvsKNNtst$statistic
pvalue
```
Como podemos apreciar, el p-valor resultante de aplicar el test de Wilcoxon es mayor que el umbral 0.05, por lo que nos revela que **no se puede rechazar la hipótesis nula** de que el algoritmo KNN tiene el mismo comportamiento que Regresión Lineal Múltiple sobre los datasets experimentados Así, se puede determinar que solo existe un 23.4% de confianza de que sean distintos. 

A continuación, **añadimos el algoritmo M5 a la comparación**. Para ello aplicamos el **test de Friedman**, que extiende el objetivo del test de Wilcoxon pero para más de dos participantes Tal y como se puede observar en los resultados, el p-valor resultante no es menor que el umbral prefijado de 0.05, por lo que **tampoco se puede rechazar la hipótesis nula de que los tres algoritmos tienen el mismo comportamiento**. Aunque no hay evidencias estadísticas de las diferencias entre las tres técnicas, vamos a agrupar los algoritmos por pares siendo 1:Regresión Lineal, 2:KNN y 3:M5. El objetivo consiste en identificar cuáles de ellos disponen de un comportamiento diferente. Con este objetivo, se aplican **múltiples tests de Wilcoxon aplicando la penalización por pasos de Holm** con la que controlar el error acumulado por realizar varios tests sobre la misma población. Como podemos observar en la tabla comparativa, **ninguno de los tests ha rechazado la hipótesis nula** de que los pares de algoritmos comparados son similares. Por lo tanto, **no existe evidencia estadística** que confirme que existen diferencias entre la Regresión Lineal Múltiple, KNN y M5.

```{r}
# Aplicamos el test de Friedman para comparar los tres algoritmos simultáneamente
test_friedman <- friedman.test(as.matrix(tablatst))
test_friedman
# Aplicamos un Post Hoc con el test de Wilcoxon y una penalización por pasos de Holm
tam <- dim(tablatst)
groups <- rep(1:tam[2], each=tam[1])
pairwise.wilcox.test(as.matrix(tablatst), groups, p.adjust = "holm", paired = TRUE)
```

# 2. Apartado de Clasificación

En este segundo apartado el dataset con que se procede a trabajar se denomina **`vehicle`**. Como en el caso anterior, comenzamos buscando información sobre sus datos y características principales. Según la descripción del repositorio [UCI Machine Learning](https://archive.ics.uci.edu/ml/datasets/Statlog+(Vehicle+Silhouettes)), este conjunto de datos está compuesto por las **propiedades de imágenes relativas a las siluetas de diferentes tipos de vehículos**. Los aspectos más destacables de este dataset se listan a continuación:

* Dispone de un total de **946 muestras y 19 atributos**. A continuación se presenta la información que he podido encontrar sobre cada uno de ellos.

| Variable | Tipo | Descripción | 
|---|---|---|
| Compactness | Entero | Relación entre el área y el volumen. Una figura compacta suele ser pequeña y redonda. 
| Circularity, Distance_circularity | Enteros | Relación entre la forma del área y del perímetro. |
| Radius_ratio | Entero | Relación entre el mínimo y máximo radio de un objeto. Ambos radios se refieren a los puntos más y menos cercanos dentro de una elipse. |
| Praxis_aspect_ratio, Max_length_aspect_ratio | Enteros | Relación entre el radio mínimo y máximo de un objeto. | 
| Scatter_ratio | Entero | Relación entre la inercia del radio mínimo y máximo.|
| Elongatedness | Entero | Relación entre el área y el ancho reducido de un objeto. |
| Praxis_rectangular, Length_rectangular | Enteros | Relación entre el área y el ancho por el alto de un objeto. |
| Major_variance, Minor_variance | Enteros | Segundo momento del radio mínimo y del radio máximo. Un momento es la captura de una imagen con unas propiedades o interpretaciones específicas. |
| Gyration_radius | Entero | Relación entre las dos propiedades anteriores.  |
| Major_skewness, Minor_skewness | Enteros | Asimetría de una imagen en ambos ejes.  |
| Minor_kurtosis, Major_kurtosis | Enteros | Forma de la imagen en ambos ejes |
| Hollows_ratio | Entero | Relación entre el área de los píxeles no activos y el perímetro de una imagen.  |
| Class | Nominal | El tipo de vehículo asociado a la silueta: van, saab, bus, opel |

* Suele ser utilizado como un **problema de clasificación** para determinar el tipo de un vehículo a partir de su contorno.

* No se afirma ni se niega la existencia de valores perdidos.

## 2.1. Análisis Exploratorio de Datos General

A continuación, se presentan las principales características de este dataset. En primer lugar, cargamos el conjunto de datos completo desde el fichero `vehicle.dat`, replicando el proceso seguido para el anterior dataset de regresión. Como podemos apreciar, este dataset dispone de **845 registros y 19 atributos**, todos numéricos a excepción de la variable `Class` que contiene valores nominales. 

```{r}
# Nombres de las variables según el fichero `vehicle.dat`
vehicle.colnames <- c("Compactness", "Circularity", "Distance_circularity", "Radius_ratio", "Praxis_aspect_ratio", "Max_length_aspect_ratio", "Scatter_ratio", "Elongatedness", "Praxis_rectangular", "Length_rectangular", "Major_variance", "Minor_variance", "Gyration_radius", "Major_skewness", "Minor_skewness", "Minor_kurtosis", "Major_kurtosis", "Hollows_ratio", "Class")
# Leemos el dataset del fichero `vehicle.dat`
# Parámetros
# 1. Ruta hacia el fichero
# 2. Las columnas no se encuentran en la primera fila
# 3. Ignoramos las 24 primeras filas hata alcanzar los datos
# 4. Separador de columnas
# 5. Nombres de las columnas
vehicle.df <- read.table("./vehicle/vehicle.dat", header=FALSE, skip=24, 
                         sep=",", col.names=vehicle.colnames)
# Dimensión 
dim(vehicle.df)
# Tipos de las variables
str(vehicle.df)
```
Como en el apartado anterior, aplicamos la función `summary` para obtener un resumen estadístico de cada una de las variables. El primer aspecto destacable de este dataset es que sus predictores disponen de **escalas de valores muy diferentes**. Esta característica se deberá tener en cuenta al construir los modelos de clasificación para que no se vean influenciados por aquellas variables con escalas mayores. La segunda propiedad más relevante es que, como en el apartado anterior, la **variable nominal `Class` también se encuentra balanceada** puesto que dispone de un número razonablemente equivalante de muestras para cada categoría. Este hecho indica que existe una representación equitativa para cada uno de los vehículos que se pretenden identificar.

Debido a que se trata de un conjunto de datos con un número mayor de variables y orientado a una temática muy particular y desconocida para mí, pese a haber buscado información sobre cada atributo, no puedo realizar un estudio tan exhaustivo como se ha efectuado con el dataset anterior. No obstante, si nos ceñimos a las medidas estadísticas, podemos observar que la **mayoría de variables disponen de intervalos de valores más amplios**, puesto sus mínimos y máximos valores se encuentran más alejados. Aunque existen algunas excepciones, como las variables `Circularity`, `Praxis_rectangular` y `Major_kurtosis` cuyos rangos de valores son de menor tamaño. 

Observando los cuartiles de cada una de las variables, podemos apreciar que en **muchos de los predictores las diferencias entre Q1 y Q3 son mínimas**. Esto indica que existe una baja variabilidad en sus datos y, por ende, una concetración de individuos con valores cercanos en los tres primeros intervalos. Un ejemplo representativo es la variable `Major_skewness`, cuyo mínimo es de 59 y su valor máximo es 135. Sin embargo, su primer cuartil es 67 mientras que su tercer cuartil tiene un valor de 75. Este hecho nos indica que la mayoría de figuras se caracterizan por una asimetría situada en el intervalo [59, 75]. Son pocas las variables que disponen de una mayor dispersión en sus datos, como son los casos de `Radius_ratio`, `Scatter_ratio`, `Gyration_radius` y `Minor_variance`, puesto que la diferencia entre sus primeros y terceros cuartiles es más considerable.

```{r}
# Resumen del dataset
summary(vehicle.df)
```
Observando la variable `Praxis_aspect_ratio` podemos apreciar que existe una concentración de vehículos cuyos radios se encuentran muy próximos, ya que la diferencia entre sus cuartiles es mínima. Considerando que estos radios parecen pertenecer a la forma de una elipse, podemos intuir que existe una **concetración de siluetas cuya altura y anchura son similares**. Como el dataset se basa en imágenes de vehículos, una posible razón para esta teoría podría hacer referencia a la existencia de una **mayor cantidad de datos sobre coches** que referentes a otros vehículos con medidas físicas más desiguales, como son las caravanas y los autobuses. Esta hipótesis puede apoyarse en dos relaciones más:

1. En la poca dispersión de dos medidas relacioandas con el área y el perímetro de un objeto: `Praxis_rectangular` y  `Length_rectangular`.

2. Y también en la poca variabilidad de las tres primeras variables asociadas a la compactación y la forma circular de una silueta.

En el caso de las medidas más familiares, podemos apreciar que tanto el grado de asimetría como de curtosis son menores en el radio mínimo, que conecta los puntos más cercanos, y mayores en el radio máximo. Esta característica parece lógica puesto que es más probable que existan **más concentraciones de datos en un espacio mayor**.

### 2.1.1. Hipótesis Iniciales

En función de las conclusiones estadísticas extraídas anteriormente, a continuación se plantea un conjunto de hipótesis iniciales sobre los datos, predictores y la variable independiente.

1. Como ocurría en el apartado anterior, en este dataset la variable `Class` dispone de un **número equivalente de muestras para cada tipo de vehículo**, por lo que sus categorías se encuentran balanceadas. Se trata de una gran ventaja puesto que favorece el entrenamiento de modelos **sin introducir el sesgo relativo a una clase predominante**.

2. Mientras que la mayoría de **predictores disponen de intervalos de valores más amplios**, no ocurre lo mismo con los **cuartiles, cuyos valores son más cercanos**. Esto significa que existe una concentración de valores similares en la mayor parte de los rangos de datos, a excepción de algunas muestras que disponen de valores más extremos. Una posible razón para este hecho es que **dos de los cuatro tipos de vehículos disponibles son marcas de automóviles**, por lo que las diferencias entre ambos pueden ser mínimas. Esta hipótesis también podría explicar la **concentración de muestras con pocas diferencias entre su radio mínimo y máximo**.

3. Finalmente, considerando que los atributos de este dataset hacen referencia a las siluetas de diversos vehículos, podemos intuir que aquellas muestras con un **mayor grado de asimetría pueden asociarse a vehículos de mayor tamaño**, como los autobuses y furgonetas. Por lo tanto, esta variable puede jugar un papel fundamental en el entrenamiento de modelos de clasificación.

## 2.2. Análisis Exploratorios Univariantes

Como en el apartado de regresión, a continuación se pretende realizar un análisis detallado para cada variable dependiendo de su tipo de datos. Para las **variables nominales representaremos la frecuencia de sus categorías** en un gráfico de barras, mientras que para los **predictores numéricos** realizaremos un estudio sobre su **varianza, desviación típica, grado de asimetría, tipo de curtosis y de distribución**.

### 2.2.1. Variables nominales

Para este dataset solo existe la única variable categórica coincide con la variable independiente que se pretende predecir. Según hemos podido observar en el resumen estadístico, la **variable `Class` está balanceada**, y tal y como podemos apreciar en el gráfico, dispone de un número muy similar de ejemplos para cada tipo de vehículo.

```{r}
# Parámetros:
# 1. Contorno negro a las barras y la leyenda
# 2. Fondo blanco
# 3. Ocultamos la etiqueta del eje X y establecemos la etiqueta del eje Y
# 4. Frecuencia de cada clase sobre sus respectivas barras
ggplot(vehicle.df, aes(x=Class, fill=Class)) +
geom_bar(color="black") + theme_minimal() +
scale_fill_discrete(name='Vehículos') +
labs(x="", y="Frecuencias") +
geom_text(stat='count', aes(label=..count..), vjust=2)
```

### 2.2.2. Variables numéricas

En la segunda parte de los análisis exploratorios individuales procedemos a calcular las medidas estadísticas comentadas anteriormente, repitiendo el proceso realizado para el dataset de regresión. 

```{r}
# Cálculo de la varianza para cada variable numérica
vehicle.vars <- sapply(c(1:(ncol(vehicle.df)-1)), function(x) var(vehicle.df[,x]))
# Cálculo de la desviación típica, asimetría y curtosis
vehicle.statistics <- data.frame(describe(vehicle.df%>%select(-Class)))
# Dataframe con todas las medidas estadísticas por variable numérica
vehicle.statistics <- vehicle.statistics %>% select(c('sd', 'skew', 'kurtosis'))
cbind(var=vehicle.vars, vehicle.statistics)
```

Tal y como predijimos en las hipótesis iniciales, la mayoría de predictores se caracterizan por una **notoria dispersión en sus datos**, siendo especialmente acusada en el caso de `Radius_ratio`, `Scatter_ratio`, `Minor_variance`, `Major_variance` y `Gyration_radius`. Este hecho explica su gran variabilidad de valores y determina que existen muestras de muy diversas características. Apoyando esta teoría podemos observar los valores de las desviaciones típicas, los cuales también son más altos para aquellos predictores con una mayor variabilidad. Como consecuencia, podemos afirmar que muchas de las **muestras disponen de valores considerablemente lejanos** a las medias para cada predictor. Existen dos posibles explicaciones para esta situación, por un lado dependiendo de los ángulos en los que se hayan tomado las imágenes para cada vehículo, las propiedades pueden variar enormemente, mientras que por otro se encuentran las diferencias físicas entre los diferentes tipos de vehículos. 

En referencia a la forma de la distribución, podemos observar que la mayoría de variables disponen de un coeficiente de asimetría por debajo de 0.5, por lo que la **mayoría de predictores parecen tener distribuciones simétricas**. Sin embargo, existen algunos casos excepcionales como `Scatter_ratio`, `Praxis_rectangular`, `Minor_variance`, `Minor_skewness` y `Minor_kurtosis` con una **moderada asimetría positiva**, por lo que estas variables se caracterizan por tener una concentración de datos a la derecha. Los casos más extremos son los predictores `Praxis_aspect_ratio`, `Max_length_aspect_ratio` y `Major_skewness` cuyos coeficientes de asimetría son superiores a 2 y por ello disponen de una **acusada asimetría positiva** con unas considerables concentraciones de muestras también a la derecha. Mediante esta medida estadística podemos intuir que existe una multitud de vehículos con valores sumamente extremos que producen distribuciones asimétricas en ciertas variables. La mayoría se encuentran asociadas con las relaciones entre las dimensiones de las imágenes y los radios de las figuras. Asimismo, el hecho de que **todas las asimetrías se encuentren a la derecha supone que se trata de valores cercanos al límite superior del intervalo**, por lo que pueden estar asociados a vehículos de mayor tamaño, como los autobuses y caravanas. 

Una situación similar ocurre con el coeficiente de curtosis. La **mayoría de predictores disponen de un valor  cercano a 0**, lo que indica que la forma de sus distribuciones pueden ser similares a la distribución normal. No es el caso de las variables `Praxis_aspect_ratio`, `Max_length_aspect_ratio` y `Major_skewness`, cuyos valores son considerablemente altos. Como consecuencia sus distribuciones presentan enormes concentraciones de datos en una zona específica del intervalo, representándose con un pico muy pronunciado.

A continuación procedemos a realizar un estudio sobre la normalidad de cada uno de los predictores. De nuevo, vamos a experimentar con un método visual para las variables más peculiares y un método estadístico para determinar con certeza la normalidad de cada predictor. Aplicaremos los gráficos QQ a las tres variables anteriores que han destacado tanto por su asimetría como por su coeficiente de curtosis.

```{r}
# Gráfico QQ para la variable dependiente `Praxis_aspect_ratio`
qqnorm(vehicle.df$Praxis_aspect_ratio, main="Gráfico QQ de Praxis_aspect_ratio")
qqline(vehicle.df$Praxis_aspect_ratio)
# Gráfico QQ para la variable dependiente `Max_length_aspect_ratio`
qqnorm(vehicle.df$Max_length_aspect_ratio, main="Gráfico QQ de Max_length_aspect_ratio")
qqline(vehicle.df$Max_length_aspect_ratio)
# Gráfico QQ para la variable dependiente `Praxis_aspect_ratio`
qqnorm(vehicle.df$Major_skewness, main="Gráfico QQ de Major_skewness")
qqline(vehicle.df$Major_skewness)
```

Como podemos apreciar, **los tres gráficos son muy similares**, sus cuantiles teóricos y empíricos coinciden razonablemente en el intervalo [-1, 1] y son diferentes al principio y especialmente al final, debido a las colas que les caracterizan a la derecha de la distribución. Debido a esta gigantesca concentración de altos valores, mi hipótesis inicial es que **ninguna de las tres variables siguen una distribución normal**. Para comprobar esta teoría y la normalidad de las restantes variables numéricas, procedemos a aplicar el test de Shapiro-Wilk’s sobre cada una. Recordemos que la hipótesis nula determina que sí sigue una distribución normal frente a la hipótesis alternativa contraria. Como podemos observar en los resultados, **todos los tests han vuelto a rechazar la hipótesis nula** puesto que sus p-valores son menores que el umbral establecido en 0.05. Así, con un 95% de evidencia estadística podemos determinar que **ninguno de los predictores de este dataset sigue una distribución normal**. 

```{r}
# Aplicamos el test de Shapiro-Wilk’s para cada variable extrayendo el p-valor
vehicle.shapiros <- sapply(c(1:(ncol(vehicle.df)-1)),
function(x) shapiro.test(vehicle.df[,x])$p.value)
# Definimos un dataframe con los resultados para cada variable
vehicle.shapiros.df <- data.frame(Variables=colnames(vehicle.df%>%select(-Class)),
PValores=vehicle.shapiros)
vehicle.shapiros.df
```

## 2.3. Análisis Exploratorio Multivariante

Como en el apartado de regresión, a continuación presentamos un conjunto de análisis globales en los que se involucran a todas las variables de este dataset. El principal objetivo consiste en visualizar y determinar las relaciones y patrones existentes entre ellos que ayuden a la predicción de la variable `Class`. Algunos de los estudios estadísticos solo admiten variables numéricas, como es el caso de las correlaciones, por lo que se añade **una columna más con los valores de `Class` codificados como etiquetas numéricas**.

```{r}
# Codificamos la variable independiente a etiquetas numéricas
# 1:bus, 2:opel, 3:saab, 4:van
vehicle.df$EncodedClass <- as.numeric(vehicle.df$Class)
```


### 2.3.1. Matriz de Puntos

A diferencia del apartado de regresión, en este caso no se puede representar una única matriz de puntos con todas las variables disponibles, puesto que su mayor número provoca que el gráfico resultante sea imposible de interpretar. Para resolver este inconveniente vamos a hacer **dos grupos de diez variables para graficar dos matrices de puntos, incluyendo en ambas la variable independiente `Class`**. Como la información disponible sobre cada predictor es mínima, he tratado de componer los **grupos según los nombres en común y sus posibles significados**, de modo que el primero de ellos contiene las medidas relativas a los diferentes ratios, mientras que en el segundo se encuentran las métricas asociadas a los ejes de las siluetas. De nuevo, **diferenciamos las muestras según el tipo de vehículo** asignándoles un color y forma diferente. A continuación se presentan las distintas asociaciones: naranja-`bus`, verde-`opel`, azul-`saab`, rosa-`van`. 

Si observamos las curvas de densidad del primer gráfico, podemos apreciar que la mayoría se caracteriza por tener varios máximos globales, lo que refuerza la teoría de que no siguen una distribución normal. Por otro lado, también se observa que en esta primera lista de predictores los valores de los **vehículos `opel` y `saab` son prácticamente similares**. Esta semejanza puede ser un problema en el entrenamiento de modelos puesto que, en este grupo, **no existen variables con los que poder diferenciarlos**. Esta situación no ocurre para los otros dos transportes de mayor tamaño, cuyos valores fluctúan más. 

Por otro lado, observando las gráficas comparativas la característica que más destaca es que en algunas se puede apreciar una clara función creciente, mientras que otras son más confusas. Existen algunas medidas en las que las **diferencias entre los distintos tipos de vehículos son mínimas**, como es el caso de `Circularity`, `Distance_circularity`, `Length_rectangular` y `Hollows_ratio`. Este hecho puede indicar que **estas variables no aportan información suficiente** como para distinguir los vehículos disponibles en este dataset, por lo que podrían ser descartadas para el entrenamiento de los modelos. No ocurre la misma situación con las variables `Praxis_aspect_ratio`, `Max_length_aspect_ratio`, `Praxis_rectangular`, `Radius_ratio` y `Scatter_ratio`, en cuyas gráficas junto con la variable independiente podemos observar diferentes valores para los distintos tipos de transportes. En los dos primeros predictores, son los **vehículos de mayor tamaño los que disponen de mayores valores**. Una posible explicación consiste en que, al ser variables que miden la relación entre el radio mínimo y máximo, pueden **distinguir aquellos vehículos de mayor longitud**. Por el contrario, en las variables  `Praxis_rectangular` y `Scatter_ratio` podemos observar que los dos tipos de automóviles disponen de los mismos rangos de valores, mientras que las furgonetas tienen los valores más bajos y los autobuses los más altos. Este hecho nos puede indicar que los **dos tipos de automóviles, al disponer de características parecidas, tienen la misma relación entre sus áreas y dimensiones**, mientras que en el caso de las furgonetas esta asociación es menor y para los autobuses es mayor. Quizás estos predictores puedan ser útiles para identificar a vehículos de mayor tamaño, como las furgonetas.

Finalmente analizamos las posibles relaciones lineales existentes entre las variables comparadas de este primer grupo. En base a los nombre de las variables, podemos intuir que **aquellas que se refieren a la misma propiedad**, como es el caso de `Circularity` y `Distance_circularity`, **pueden compartir una relación monotónica** y por lo tanto pueden aporta la misma información. En este caso, bastaría con seleccionar la que más información aporte para descartar las restantes en la construcción de los modelos de clasificación. Asimismo, a excepción de las gráficas comparativas con `Hollows_ratio`, en la mayoría se puede observar relaciones crecientes puesto que en sus representaciones los valores continuan aumentando a lo largo del intervalo. Este hecho puede indicarnos que la **mayoría de predictores se encuentran relacionados linealmente**, por lo que contienen una gran cantidad de información común.

```{r}
# Primer grupo de variables para representar una matriz de puntos
vehicle.first_group <- c("Circularity", "Distance_circularity", "Praxis_aspect_ratio",
                         "Max_length_aspect_ratio", "Praxis_rectangular", 
                         "Length_rectangular", "Radius_ratio", "Scatter_ratio", 
                         "Hollows_ratio", "EncodedClass")
# Parámetros
# 1. Representa solo la diagonal inferior porque la matriz es simétrica
# 2. Diferencia los ejemplares por género cambiando el color y su forma
# 3. Representa curvas de densidad en la diagonal
scatterplotMatrix(vehicle.df[vehicle.first_group], diagonal=list(method="density"),
upper.panel=NULL, col=c("orange", "green", "blue", "pink"),
group=as.factor(vehicle.df$Class))
```

En esta segunda matriz de puntos se representan los restantes predictores incluyendo, también, la variable independiente `Class`. De nuevo, si observamos sus curvas de densidad podemos visualizar que la mayoría de ellas se caracterizan por dos propiedades: varios valores máximos globales y una **gran similitud entre los dos tipos de automóviles**. Adicionalmente, si examinamos las gráficas comparativas de los predictores con la variable a predecir, podemos apreciar que en **más de la mitad los valores son similares para los cuatro tipos de vehículos**. De nuevo, estos predictores parecen no ayudar en la identificación de cada uno de los transportes disponibles, por lo que quizás puedan ser eliminados de los modelos. Las cinco variables que pueden ser de mayor utilidad se listan a continuación:

* Los predictores `Minor_variance` y `Compactness` disponen de valores inferiores para el tipo de vehículo `van`, mientras que los restantes transportes disponen de rangos de valores más amplios, lo cual parece indicar que las **furgonetas disponen de siluetas más esféricas** que los demás vehículos. Además, en el caso de la última variable, podemos observar que junto con `Major_kurtosis` son las únicas en las que existen ciertas diferencias entre los dos tipos de automóviles. Por lo que estos dos predictores pueden aportar cierta información que permita su distinción.

* La variable `Major_skewness` contiene un intervalo de valores mayor para los vehículos de mayor tamaño que para los dos tipos de automóviles. Este predictor puede ayudar a **distinguir entre estos dos grupos de transportes**, ya que las siluetas de los autobuses y furgonetas suelen estar caracterizadas por una mayor asimetría.

* Por último, la variable `Elongatedness` dispone de valores inferiores para el tipo de vehículo `bus` en contraposición con los valores más altos asociados al tipo de transporte `van`. Esto indica que las **siluetas de las furgonetas son las más estiradas**, por lo que este predictor puede proporcionar información útil con la que diferenciar los dos tipos de vehículos de mayor tamaño.

Por último, procedemos a estudiar las posibles relaciones lineales entre los diferentes predictores disponibles. A diferencia de la matriz de puntos anterior, en la mayoría de las gráficas no se reconoce ninguna función particular que determine el aumento o disminución de los valores, por lo que **parece existir un menor número de asociaciones**. Las más destacables son las relaciones lineales entre las variables `Major_variance`, `Minor_variance` en combinación con `Gyration_radius`, `Compactness` y `Elongatedness`. Existen **cuatro relaciones monotónicas decrecientes** entre esta última variable y las cuatro restantes mencionadas anteriormente. En primer lugar se encuentran las tres asociadas con los momentos de las imágenes en el radio mínimo y máximo, que parecen indicar que mayor valor, menor es la elasticidad de la silueta. Mientras que para la variable `Compactness` parece que conforme menos elástica es la silueta, menos estirada es, característica que parece lógica si consideramos la elasticidad de un círculo perfecto.

```{r}
# Segundo grupo de variables para representar una segunda matriz de puntos
vehicle.second_group <- c("Major_variance", "Minor_variance", "Major_skewness", 
                          "Minor_skewness", "Minor_kurtosis", "Major_kurtosis", 
                          "Gyration_radius", "Compactness", "Elongatedness", 
                          "EncodedClass")
# Parámetros
# 1. Representa solo la diagonal inferior porque la matriz es simétrica
# 2. Diferencia los ejemplares por género cambiando el color y su forma
# 3. Representa curvas de densidad en la diagonal
scatterplotMatrix(vehicle.df[vehicle.second_group], diagonal=list(method="density"),
upper.panel=NULL, col=c("orange", "green", "blue", "pink"),
group=as.factor(vehicle.df$Class))
```

### 2.3.2. Correlaciones

A continuación se representarán dos gráficas de correlaciones para cada uno de los diferentes grupos anteriores con el objetivo de identificar posibles asociaciones entre los predictores y con la variable independiente. Así, se pretende conocer cuáles son las variables más prometedoras para predecir el tipo de vehículo a partir de las propiedades de su silueta. Tal y como podemos observar en el siguiente gráfico, nuestra teoría inicial de que podría haber numerosas relaciones entre los predictores parece cumplirse. En particular, las siguientes asociaciones son las más relevantes:

1. En primer lugar se encuentran los predictores **`Circularity` y `Distance_circularity`** con un coeficiente de correlación de 0.8, lo que indica una **relación alta**. Sin embargo, esta asociación era de esperar puesto que ambas variables hacen referencia a la misma propiedad. Esto nos permitirá descartar a la que aporte menos información útil para la predicción de la variable independiente. **En esta misma situación se encuentran la pareja de predictores `Length_rectangular` y `Praxis_rectangular`**. Adicionalmente, podemos observar que **las cuatro variables están relacionadas entre sí** por unos coeficientes de correlación entre 0.77 y 0.97, por lo que comparten una considerable cantidad de información. Particularmente, la correlación entre **`Circularity` y `Length_rectangular` afirma que existe una asociación perfecta**. Una posible razón explicativa es que en las fórmulas de ambos predictores se consideran **el área y el perímetro de la silueta**, dos medidas matemáticas muy relacionadas entre sí.

2. Un segundo grupo de variables correladas lo forman aquellas que hacen referencia a una proporción o ratio entre varias medidas. Especialmente destacan los predictores **`Scatter_ratio` y `Praxis_rectangular` con un coeficiente de 0.99, lo que indica que existe una relación total** entre sendas variables. Mientras la primera hace referencia a la inercia entre el radio mínimo y máximo, la segunda vincula el área con las dimensiones de la silueta. Aunque parezcan medidas diferentes, parecen aportar la misma información por lo que se podrá descartar aquella que menos relevante sea para el entrenamiento de modelos de clasificación. Otras correlaciones que se pueden categorizar como moderadas las protagonizan los pares de variables `Scatter_ratio` y `Radius_ratio`, `Praxis_aspect_ratio` y `Max_length_aspect_ratio`.

3. Adicionalmente, **algunas de las variables de los dos grupos anteriores también se encuentran correladas** entre sí. La relación más destacable por su fuerza es la de **`Scatter_ratio` y `Distance_circularity` con un coeficiente de 0.91**, lo que indica que sendos predictores también aportan el mismo tipo de información. LA explicación de este suceso puede ser similar a la anterior y es que la primera variable hace referencia a los radios de la silueta, mientras que la otra relaciona el área con su volúmen, por lo que los componentes de cada medida matemática están fuertemente vinculados.

Finalmente, destacamos la **ausencia de correlaciones importantes con la variable independiente**, por lo que en este primer grupo no parece haber predictores particulares que expliquen la variabilidad de vehículos de este dataset.

```{r}
# Representa los coeficientes de correlación para el 
# primer grupo de predictores y la variable independiente
# Parámetros
# 1. Calcula los coeficientes de correlación
# 2. Representa los coeficientes con números
# 3. Orden alfabético de las variables
# 4. Representa solo la matriz diagonal inferior
# 5. Tamaño de las etiquetas y de los coeficientes de correlación
corrplot(cor(vehicle.df%>%select(vehicle.first_group)), 
         method='number', order='alphabet',
type='lower', tl.cex=0.7, number.cex=0.7)
```

En este segundo gráfico se muestran las correlaciones existentes entre las variables del segundo grupo más la variable independiente. La primera cualidad destacable es que, a diferencia del gráfico anterior, la **mayoría de correlaciones importantes son negativas**, lo que indica que los predictores relacionados no comparten una asociación monotónica, si no un vínculo inverso, puesto que conforme aumenta uno disminuye el contrario, y viceversa. En este caso vamos a explicarlas según la obviedad de las mismas.

1. La primera asociación más trivial es la compuesta por **`Compactness` y `Elongatedness` puesto que se trata de dos variables opuestas**. Mientras la primera mide el grado de compactación de una silueta, entendiendo esta propiedad como el grado de similitud con un círculo perfecto, el segundo predictor representa cuán elástica es la silueta. Por lo tanto, su asociación contraria parece lógica. 

2. Una segunda correlación positiva bastante obvia es la que forman las variables `Minor_variance` y `Major_variance`, puesto que ambas representan la **misma medida pero para diferentes longitudes de radio. Esta relación se representa con una tercera variable denominada `Gyration_radius`** y por ende, genera dos correlaciones adicionales con los dos primeros predictores. Esto nos indica que probablemente solo necesitemos el tercer predictor para representar toda la información que aportan estas propiedades.

3. Si existen varias correlaciones positivas moderadas y fuertes entre las variables `Compactness` y los tres predictores del punto anterior, es lógico encontrar **correlaciones negativas las tres variables anteriores y el predictor opuesto `Elongatedness`**. Sin embargo, estas relaciones contrarias parecen ser mucho más fuertes que las asociaciones positivas puesto que sus coeficientes son considerablemente mayores. Esto nos indica que comparten más información con el predictor `Elongatedness` que con la variable `Compactness`.

4. La última correlación a destacar la conforman `Major_skewness` y `Major_kurtosis`. Se trata de una **relación fuerte y en dirección opuesta** por su coeficiente negativo. A diferencia de los casos anteriores, esta relación no ha sido obvia a mi parecer debido a los conceptos estadísticos de asimetría y curtosis por los que me estaba guiando para intentar entender ambas variables. Sin embargo, en el caso de este dataset, parece que cuanto más asimétrica sea una silueta, menor coeficiente de curtosis, y viceversa. 

Como en el gráfico anterior, podemos observar que en segundo **tampoco existen correlaciones relevantes con la variable independiente**, por lo que parece difícil explicar las siluetas de los vehículos con variables particulares. Deberemos de formar un conjunto de métricas que en conjunto aporten información suficiente para identificar los diferentes transportes disponibles.

```{r}
# Representa los coeficientes de correlación para el 
# segundo grupo de predictores y la variable independiente
# Parámetros
# 1. Calcula los coeficientes de correlación
# 2. Representa los coeficientes con números
# 3. Orden alfabético de las variables
# 4. Representa solo la matriz diagonal inferior
# 5. Tamaño de las etiquetas y de los coeficientes de correlación
corrplot(cor(vehicle.df%>%select(vehicle.second_group)), 
         method='number', order='alphabet',
type='lower', tl.cex=0.7, number.cex=0.7)
```


### 2.3.3. Diagramas de cajas

Para acabar el estudio multivariante de este dataset, a continuación procedemos a representar los diagramas de cajas de cada uno de los grupos anteriores para mostrar su dispersión y variabilidad de datos. Como hemos analizado anteriormente, a diferencia del dataset de regresión, las variables de **este conjunto de datos sí se caracterizan por escalas súmamente diferentes**, lo que puede influir tanto en este tipo de gráficos como en los clasificadores. Por ende, en primer lugar aplicamos la función `scale` para utilizar un **escalado y centrado mediante *Z score** con el que construir un segundo dataset cuyos datos dispongan de una media 0 y desviación típica de 1. 

En este primer gráfico podemos observar que la **mayoría de predictores se caracterizan por una moderada o severa asimetría**, según se ha estudiado en secciones anteriores, a excepción de las variables `Scatter_ratio`, `Max_length_aspect_ratio` y `Distance_circularity`, que disponen de niveles más bajos puesto que su mediana se sitúa aproximadamente en el centro de las cajas. Otra característica a destacar es que, como se predijo anteriormente, la **mayoría de predictores disponen de una gran dispersión en sus datos** en base a la longitud de sus correspondientes cajas. Esta hipótesis también ha sido estudiada anteriormente de forma generalizada y específica a las variables según la naturaleza de sus valores. No obstante, aún queda un tipo de análisis que no se ha efectuado aún: los valores outliers. Como podemos apreciar, **solo tres de las variables representadas disponen de muestras alejadas del conjunto de valores**. En el caso del predictor `Scatter_ratio` podemos observar que se trata de valores no demasiado alejados del valor máximo del conjunto. Sin embargo, los predictores `Praxis_rectangular` y `Max_length_aspect_ratio` sí que contienen datos extremadamente alejados del rango de valores habitual, por lo que estas variables pueden contar con un conjunto de outliers extremos. Las **tres variables hacen referencia a los radios mínimo y máximo** de una silueta, por lo que considerando que dos de los cuatro vehículos disponibles son automóviles, podemos intuir que estas muestras tan alejadas pueden referirse a **vehículos con siluetas de mayor tamaño**, como puede ser el caso de los autobuses.

```{r}
# Escalamos y centramos todas las variables numéricas
vehicle.scaled.df <- data.frame(scale(vehicle.df 
                                      %>% select(-Class, -EncodedClass)))
# Añadimos las variables de clase y la codificada al dataset escalado
vehicle.scaled.df$Class <- vehicle.df$Class
vehicle.scaled.df$EncodedClass <- vehicle.df$EncodedClass

# Definimos un color por cada variable
var_colors<-c("red", "yellow", "orange", "green", "cyan",
              "blue", "pink", "purple", "gray")
# Diagrama de cajas para el primer grupo de predictores, 
# sin la variable independiente
# Parámetros
# 1. Escala al 90% del tamaño del nombre de las variables y del gráfico
# 2. Conjunto de datos a representar
# 3. Representa la asimetría de los datos
# 4. Colores para cada caja
# 5. Esconde el eje X y personaliza el eje Y para que el gráfico sea horizontal
# y muestre los nombres de las variables para cada caja
par(cex.axis=0.9, mar=c(1, 10, 1, 1))
boxplot(vehicle.scaled.df %>% select(vehicle.first_group, -EncodedClass), 
        notch=TRUE, col=var_colors, 
        xaxt="n", yaxt="n", horizontal=TRUE)
axis(2, labels=vehicle.first_group[1:(length(vehicle.first_group)-1)], 
     at=1:(length(vehicle.first_group)-1), las=2)
```

A continuación se muestra un segundo gráfico con las restantes variables que no se mostraban en el anterior. Como ocurría en el primer diagrama de cajas, en esta segunda representación existen **ciertas variables con un mayor grado de asimetría**, mientras que otras disponen de una mediana bastante centrada, como es el caso de `Compactness`. De igual modo, podemos visualizar que la mayoría de predictores disponen tanto de un amplio rango de valores como de una considerable longitud de sus cajas, por lo que sus **datos también se encuentran considerablemente dispersos**. En el caso de los valores extremos, podemos observar que a diferencia de las variables anteriores, que los predictores de este gráfico **la mayoría solo dispone de outliers moderados**, es decir, puntos fuera del intervalo de valores pero bastante cercanos a él, a excepción de `Major_skewness`, que sí que dispone de un conjunto de muestras extremadamente alejadas de su respectiva caja. Como hemos analizado anteriormente, esta variable era capaz de diferenciar dos grupos de transportes: los dos tipos de automóviles y los dos vehículos de mayor tamaño. Por tanto, con gran probabilidad, los **outliers extremos que tiene harán referencia a enormes grados de asimetrías** asociados a este último grupo.  

```{r}
# Diagrama de cajas para el segundo grupo de predictores, 
# sin la variable independiente
# Parámetros
# 1. Escala al 90% del tamaño del nombre de las variables y del gráfico
# 2. Conjunto de datos a representar
# 3. Representa la asimetría de los datos
# 4. Colores para cada caja
# 5. Esconde el eje X y personaliza el eje Y para que el gráfico sea horizontal
# y muestre los nombres de las variables para cada caja
par(cex.axis=0.9, mar=c(1, 10, 1, 1))
boxplot(vehicle.scaled.df %>% select(vehicle.second_group, -EncodedClass), 
        notch=TRUE, col=var_colors, 
        xaxt="n", yaxt="n", horizontal=TRUE)
axis(2, labels=vehicle.second_group[1:(length(vehicle.second_group)-1)], 
     at=1:(length(vehicle.second_group)-1), las=2)
```

