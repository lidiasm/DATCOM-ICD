---
title: "Ejercicios de Clasificación"
author: "Lidia Sánchez Mérida"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

En primer lugar, cargamos la librerías necesarias para realizar los ejercicios de clasificación y establecemos una **semilla inicial para que los resultados puedan ser reproducibles**.

```{r message=FALSE, warning=FALSE}
# Cargamos las librerías necesarias para todos los ejercicios
library(tidyverse)
library(philentropy)
library(gridExtra)  # Librería para combinar diferentes gráficos en una sola vista
library(caret)
library(corrplot)
library(ISLR)
library(car)

# Semilla inicial
set.seed(0)
```

# Ejercicio 1

En este primer ejercicio el objetivo consiste en reproducir el comportamiento del algoritmo `KNN`, extendiendo su capacidad de aplicar diversas métricas para calcular las distancias entre las muestras a clasificar y las del conjunto de entrenamiento. Los parámetros que acepta esta función son:

* `train`: es el conjunto de muestras **sin la columna a predecir** con el que se pretende realizar el entrenamiento del modelo.
* `train_labels`: se trata de la variable a predecir con la que entrenar un nuevo modelo.
* `test`: se trata del conjunto de validación **sin la columna a predecir** para evaluar la bondad del modelo y realizar las predicciones. Es un parámetro opcional, si no se proporciona, se tomará una porción del conjunto de entrenamiento para realizar la validación y las predicciones.
* `test_labels`: se trata de la variable a predecir con la que evaluar el modelo entrenado. Es un parámetro opcional, si no se especifica se tomarán las etiquetas correspondientes al conjunto de test obtenido del conjunto de entrenamiento.
* `k`: es el número de vecinos que se considerará para clasificar las muestras del conjunto de validación. Es un parámetro opcional, por defecto será 1.
* `metric`: se trata de la métrica que se utilizará para calcular las distancias entre las muestras de entrenamiento y las de validación. Es un parámetro opcional, por defecto se aplicará la distancia Euclídea.

```{r}
my_knn <- function(train, train_labels, test=NA, test_labels=NA, k=1, metric="euclidean") {
  new_train<-train
  new_test<-test
  # 90%-10% en caso de que no se haya especificado un conjunto de test
  if (is.na(test)) {
    shuffle_train <- sample(dim(train)[1])
    pct90 <- (dim(train)[1] * 90) %/% 100
    new_train <- train[shuffle_train[1:pct90], ]
    train_labels <- train_labels[shuffle_train[1:pct90]]
    new_test <- train[shuffle_train[(pct90+1):dim(train)[1]], ]
    test_labels <- train_labels[shuffle_train[(pct90+1):dim(train)[1]]]
  }
  # Vector de predicciones
  preds <- c()
  # Calculamos la matriz de distancia entre cada muestra de test y todas las de train
  for(i in 1:dim(new_test)[1]) {
    dist_mat <- c()
    for(j in 1:dim(new_train)[1]) {
      xmat <- rbind(new_test[i,], new_train[j,])
      # Silencia la salida de la función `distance` para luego imprimir la tasa de aciertos
      dist_mat <- append(dist_mat, invisible(suppressMessages(distance(as.data.frame(xmat), metric)))) 
    }
    # Obtenemos las K muestras con menor distancia 
    neighs <- sort(dist_mat)[1:k] 
    # Realizamos una votación para decidir la clase a la que pertenece la muestra de test.
    # Para ello se realiza una media de las clases de las muestras más cercanas y se 
    # redondea el valor para obtener un número entero.
    sample_class <- 0
    for(s in 1:length(neighs)) {
      pos <- which(dist_mat == neighs[s])
      sample_class <- sample_class + train_labels[pos]
    }
    preds <- append(preds, round(sample_class/k))
  }
  
  # Calculamos la tasa de aciertos comparando las predicciones con las etiquetas
  # reales del conjunto de test
  cat("\nTasa de aciertos: ", mean(preds == test_labels, na.rm=TRUE))
  
  # Devuelve una lista con un vector que contiene las predicciones sobre el conjunto de test
  return (preds)
}
```

A continuación, aplicamos la función definida anteriormente al dataset `BreastCancer` para entrenar un nuevo modelo y obtener las predicciones de la variable `diagnosis`. Para ello, cargamos el dataset, eliminamos la columna `id` puesto que no aporta información, así como la columna `diagnosis` para luego proporcionarla por separado. Previo a la ejecución de la función, **escalamos y centramos** el conjunto de datos y **codificamos las categorías de la variable `diagnosis`** a etiquetas numéricas para realizar la votación de la clase a asignar a cada muestra del conjunto de test.

En este ejercicio se han realizado cuatro experimentos diferentes. En los dos primeros se aplica la **distancia Euclídea** con dos diferentes valores de K, mientras que en los dos últimos se utiliza la **distancia de Manhattan**. He seleccionado esta métrica puesto que he leído en diversas fuentes (https://towardsdatascience.com/9-distance-measures-in-data-science-918109d069fa) que suele utilizarse para variables discretas o binarias, como es el caso de este dataset. Así, el objetivo consiste en comprobar el comportamiento del algoritmo con diferentes distancias y número de vecinos más cercanos.

```{r}
### SERVIDOR EN MANTENIMIENTO ###
# wbcd <- read.csv("https://resources.oreilly.com/examples/9781784393908/raw/ac9fe41596dd42fc3877cfa8ed410dd346c43548/Machine%20Learning%20with%20R,%20Second%20Edition_Code/Chapter%2003/wisc_bc_data.csv")
# Cargamos el dataset desde un fichero
wbcd <- read.csv("wisc_bc_data.csv")
# Eliminamos el identificador de los registros porque no aporta información
# Eliminamos la columna `diagnosis` con las predicciones porque se proporciona como parámetro independiente
new_wbcd <- wbcd %>% select(-id) %>% select(-diagnosis)
# Escalamos y centramos el conjunto de datos
scaled_wbcd <- new_wbcd %>% mutate_if(is.numeric, scale, center = TRUE, scale = TRUE)

### EXPERIMENTOS ###
# Entrenamos un primer modelo con la distancia Euclídea y k=3
diagnosis_labels <- as.numeric(wbcd$diagnosis, levels=unique(wbcd$diagnosis))
first_knn_model <- my_knn(scaled_wbcd, diagnosis_labels, k=3)
# Entrenamos un segundo modelo con la distancia Euclidea y k=21
second_knn_model <- my_knn(scaled_wbcd, diagnosis_labels, k=21)
# Entrenamos un tercer modelo con la distancia Manhattan y k=3
third_knn_model <- my_knn(scaled_wbcd, diagnosis_labels, k=3, metric="manhattan")
# Entrenamos un cuarto modelo con la distancia Manhattan y k=9
fourth_knn_model <- my_knn(scaled_wbcd, diagnosis_labels, k=21, metric="manhattan")

# Creamos un dataset con las predicciones de los cuatro modelos para representarlas en una comparativa
knn_df <- data.frame(FirstModel=first_knn_model, SecondModel=second_knn_model, ThirdModel=third_knn_model, FourthModel=fourth_knn_model)
# Modificamos las etiquetas de numéricas a las categorías 'Benign' y 'Malignant'
knn_df <- knn_df %>% mutate(FirstModel=factor(FirstModel, labels = c("Benign", "Malignant"))) %>% mutate(SecondModel=factor(SecondModel, labels = c("Benign", "Malignant"))) %>% mutate(ThirdModel=factor(ThirdModel, labels = c("Benign", "Malignant"))) %>% mutate(FourthModel=factor(FourthModel, labels = c("Benign", "Malignant")))
# Aplicamos la función summary() para ver la proporción de las predicciones realizadas por cada modelo
cat("\n-----------------------------------------------------------\n")
summary(knn_df)
```
Como se puede comprobar, la tasa de aciertos de los **dos modelos que utilizan la distancia Euclídea ronda los 48% y 42%, respectivamente**. Con un valor de K más pequeño, el algoritmo parece proporcionar mejores resultados aunque observando la proporción de las predicciones podemos apreciar que son bastante similares. 
Mientras que en el caso de los **modelos que aplican la distancia Manhattan**, podemos observar que sus **tasas de aciertos son considerablemente superiores** utilizando los mismos valores de K que los modelos con la distancia Euclídea. Esto nos indica que este tipo de métrica ayuda al algoritmo a generalizar mejor y acertar más para este dataset en concreto. Sin embargo, de nuevo podemos visualizar que con un valor de K más pequeño se consiguen mejores resultados que con un valor mayor. Por lo tanto, para este dataset, el algoritmo KNN necesita considerar un menor número de vecinos para ajustarse más a los datos de entrenamiento.

Por último, se representan los resultados obtenidos en los cuatro modelos con cuatro gráficos de barras en los que se reflejan el número de muestras clasificadas como benignas o malignas. Como se había comentado anteriormente, las predicciones de los dos primeros modelos son sumamente similares entre sí. A diferencia de las predicciones de los modelos que utilizan la distancia Manhattan, en los que se aprecia un aumento de muestras clasificadas como benignas y un decremento de las muestras de la clase opuesta, especialmente en el caso del tercer modelo con un K menor. Este hecho puede tener sentido si consideramos que el dataset dispone de **más muestras benignas que malignas** por lo que existe una mayor probabilidad de que los vecinos más cercanos a la muestra a clasificar pertenezcan a la clase mayoritaria.

```{r}
# Representamos las predicciones de los cuatro modelos en un grid 2x2
g1 <- ggplot(data=knn_df, aes(x=FirstModel, fill=FirstModel)) + geom_bar() + xlab("Diagnóstico") + ylab("Frecuencias")
g2 <- ggplot(data=knn_df, aes(x=SecondModel, fill=SecondModel)) + geom_bar() + xlab("Diagnóstico") + ylab("Frecuencias")
g3 <- ggplot(data=knn_df, aes(x=ThirdModel, fill=ThirdModel)) + geom_bar() + xlab("Diagnóstico") + ylab("Frecuencias")
g4 <- ggplot(data=knn_df, aes(x=FourthModel, fill=FourthModel)) + geom_bar() + xlab("Diagnóstico") + ylab("Frecuencias")
grid.arrange(g1, g2, g3, g4, nrow=2, ncol = 2)
```

# Ejercicio 2

En este segundo ejercicio se pretende entrenar varios modelos de **Regresión Logística utilizando validación cruzada** sobre el dataset que se está utilizando hasta el momento: `BreastCancer`. Para ello vamos a utilizar la librería `caret` que permite el entrenamiento de diversos modelos con esta técnica.

En el primer experimento se pretenden usar todos los predictores disponibles a excepción de `id` puesto que no aporta información útil para predecir la variable `diagnosis`. Como podemos observar en los resultados, **considerando todos los predictores disponibles el algoritmo no converge**. 

```{r}
# Primer modelo de Regresión Logística utilizando todos los predictores excepto `id`
wbcd_glm1 <- train(new_wbcd, y=wbcd$diagnosis, method = "glm", tuneLength = 10, trControl = trainControl(method = "cv"))
```
Esta salida puede ser un indicador de la existencia de correlaciones entre las variables definidas en la fórmula del modelo. Por curiosidad, a continuación se muestra un gráfico con las correlaciones entre todas las variables consideradas. Para su mejor visualización, se representa solo la diagonal inferior con un intervalo de colores y ordenando las variables alfabéticamente. 

```{r}
# Correlación entre las variables consideradas en el primer modelo
corrplot(cor(new_wbcd), method='color', order='alphabet', type='lower')
```
Como podemos apreciar, existen varios grupos de variables con una alta correlación, como aquellas medidas relativas al área, perímetro y radio. Si quisiésemos entrenar un buen modelo, deberíamos realizar un estudio acerca de qué variables elegir y cuáles descartar maximizando la cantidad de información relativa a la variable a predecir `diagnosis`. Sin embargo este no es el objetivo del ejericio por lo que se define una fórmula cualquiera, considerando levemente las correlaciones mostradas anteriormente, para poder aplicar la validación cruzada con Regresión Logística.

```{r}
# Segundo modelo de Regresión Logística utilizando algunos predictores
wbcd_glm2 <- train(new_wbcd %>% select(c(area_mean, compactness_worst, concavity_se, smoothness_mean, symmetry_mean)), y=wbcd$diagnosis, method="glm", tuneLength=10, trControl=trainControl(method="cv"))
wbcd_glm2
```
Como podemos observar el mejor modelo resultante de la validación cruzada dispone de un 92% de precisión, aproximadamente, lo cual puede indicar que tiene una buena capacidad de generalización para predecir la clase de muestras desconocidas.

# Ejercicio 3

En este ejercicio el objetivo consiste en entrenar diversos modelos con **LDA, Regresión Logística y QDA** para comparar su comportamiento sobre el dataset `Smarket` utilizando todas las variables `Lag`. Comenzamos con la aplicación del algoritmo **LDA**. Para ello, vamos a comprobar que se cumplen las siguientes condiciones:

1. Que todos los predictores siguen una distribución normal. Para ello aplicamos el **test de Shapiro para cada variable sobre cada clase**. Como podemos observar en los resultados, todos los p-valores obtenidos son menores que el umbral=0.05 por lo que **se rechaza la hipótesis nula de que sigan una distribución normal**.

2. Que todos los predictores tienen la misma covarianza. Tras conocer que ninguna variable sigue una distribución normal con respecto a cada clase, aplicamos el **test de Levene para cada predictor con respecto a la variable independiente**. De nuevo, todos los p-valores obtenidos son mayores que el umbral=0.05 por lo que **se rechaza la hipótesis nula de que tengan la misma covarianza**.

```{r}
# Aplicamos el test de Shapiro para verificar la normalidad para cada predictor y cada clase
up <- Smarket %>% filter(Direction == "Up")
down <- Smarket %>% filter(Direction == "Down")
shapiro.test(up$Lag1)
shapiro.test(down$Lag1)
shapiro.test(up$Lag2)
shapiro.test(down$Lag2)
shapiro.test(up$Lag3)
shapiro.test(down$Lag3)
shapiro.test(up$Lag4)
shapiro.test(down$Lag4)
shapiro.test(up$Lag5)
shapiro.test(down$Lag5)
# Aplicamos el de test de Levene para comprobar si todos los predictores tienen la misma covarianza
leveneTest(Lag1 ~ Direction, Smarket)
leveneTest(Lag2 ~ Direction, Smarket)
leveneTest(Lag3 ~ Direction, Smarket)
leveneTest(Lag4 ~ Direction, Smarket)
leveneTest(Lag5 ~ Direction, Smarket)
```

Una vez conocemos que ninguna de las dos condiciones anteriores se cumplen para los predictores elegidos, entrenamos el modelo mediante LDA para comprobar su comportamiento. Para ello dividimos el conjunto de datos en en **entrenamiento (2001-2004) y en validación (2005)**, como los scripts de ejemplo de la asignatura. Como podemos era de esperar, solo propociona un **53% de tasa de aciertos** por lo que las predicciones sobre el conjunto de test se han realizado, prácticamente, de forma aleatoria. 

```{r}
# Dividimos el conjunto de datos en entrenamiento (2001-2004) y validación (2005)
smkt_train <- subset(Smarket, Year>=2001 & Year<=2004) %>% select(c(Lag1, Lag2, Lag3, Lag4, Lag5, Direction))
smkt_test <- subset(Smarket, Year=2005) %>% select(c(Lag1, Lag2, Lag3, Lag4, Lag5, Direction))
# Primer modelo entrenado con LDA 
smkt_lda <- lda(Direction~., data=smkt_train)
# Obtenemos las predicciones sobre el conjunto de test
smkt_lda_preds <- predict(smkt_lda, smkt_test)
# Calculamos la tasa de aciertos
mean(smkt_lda_preds$class==smkt_test$Direction)
```
A continuación probaremos con un segundo modelo entrenado a partir de **QDA**, cuyas condiciones son las mismas que en el caso anterior a excepción de la igualdad de varianza. Reutilizamos los conjuntos de entrenamiento y validación anteriores. Sin embargo, la tasa de acierto solo aumenta un 2%, por lo que una **función cuadrática tampoco es capaz de separar las clases de este dataset**.

```{r}
# Segundo modelo entrenado con QDA
smkt_qda <- qda(Direction~., data=smkt_train)
# Obtenemos las predicciones sobre el conjunto de test
smkt_qda_preds <- predict(smkt_qda, smkt_test)
# Calculamos la tasa de aciertos
mean(smkt_qda_preds$class==smkt_test$Direction)
```
Finalmente entrenaremos un tercer modelo usando **Regresión Logística**. Para este algoritmo las condiciones que se deben verificar son:

1. Las variables predictoras deben presentar una **baja o nula correlación** entre sí. Como se puede apreciar en el primer gráfico, esta condición parece que se cumple puesto que no existen coeficientes de correlación significativos. 

2. **No deben existir valores extremos o *outliers* **. Para esta segunda condición representamos los diagramas de cajas para cada predictor y, tal y como se observa en el segundo gráfico, todos ellos tienen una cantidad considerable de valores extremos, por lo que esta condición no se cumple.

```{r}
# Correlación entre las variables `Lag`
corrplot(cor(Smarket %>% select(c(Lag1, Lag2, Lag3, Lag4, Lag5))), method='color', order='alphabet')
# Outliers en las variables `Lag`
boxplot(Smarket %>% select(c(Lag1, Lag2, Lag3, Lag4, Lag5)), col="orange")
```
Tras verificar que solo se cumple una condición de las dos exigidas, entrenamos el modelo con Regresión Logística y como era de esperar también proporciona una tasa de aciertos similar a los otros dos modelos con un **53% de precisión**. Por lo tanto, este algoritmo tampoco es el adecuado para clasificar las muestras de este conjunto de datos.

```{r}
# Tercer modelo entrenado con Regresión Logística
smkt_glm <- train(smkt_train %>% select(-Direction), y=smkt_train$Direction, method="glm")
# Obtenemos las predicciones sobre el conjunto de test
smkt_glm_preds <- predict(smkt_glm, smkt_test)
# Calculamos la tasa de aciertos
mean(smkt_glm_preds==smkt_test$Direction)
```
# Ejercicio 4

En este último ejercicio se pretende realizar una comparación entre los resultados obtenidos al entrenar diferentes modelos con diversos datasets. Comenzamos utilizando el **test de Wilcoxon** para determinar si los modelos entrenados con LDA son significativamente diferentes a los entrenados con QDA. Para ello establecemos como **hipótesis nula que son iguales**, y como hipótesis alternativa que son diferentes. Como se puede apreciar en los resultados, el p-valor devuelto es mayor que el umbral (0.05) por lo tanto **no se puede rechazar la hipótesis nula** y, en consecuencia, no se puede determinar que el comportamiento de sendos algoritmos es el mismo.

A continuación se aplica el **test de Friedman** para realizar una comparación múltiple entre los tres algoritmos disponibles: KNN, LDA y QDA. En este test la **hipótesis nula afirma que sus comportamientos son iguales**, frente a la hipótesis alternativa que sostiene que son diferentes. De nuevo, el p-valor resultante es mayor que el umbral (0.05) por lo tanto se **rechaza la hipótesis nula**. Así, podemos indicar que los comportamientos de los tres algoritmos no son iguales al 95% de confianza.

```{r}
# Cargamos los resultados de aplicar diferentes clasificadores a diversos datasets
clasif_models <- read.csv("clasif_train_alumnos.csv")
# Test de Wilcoxon: LDA vs. QDA
lda.vs.qda <- wilcox.test(clasif_models$out_train_lda, clasif_models$out_train_qda)
lda.vs.qda
# Test de Friedman: LDA vs. QDA vs. KNN
friedman.test(data.matrix(clasif_models %>% select(-X)))
```
Finalmente, aunque el test de Friedman ha rechazado la hipótesis nula, vamos a aplicar un Post Hoc para comparar a todos los algoritmos entre sí efectuando **pares de tests (LDA vs. QDA, QDA vs. KNN, LDA vs. KNN)**. Para cada una de las combinaciones utilizaremos un **test de Wilcoxon** de modo que devuelva los p-valores sin tener en cuenta el `FWER`, que es el error acumulado que se comete al realizar varios tests simultáneos sobre los mismos datos. Posteriormente, lo tendremos en consideración aplicando una **penalización de Holm con la que calcular unos nuevos p-valores ajustados al `FWER`**. Como podemos comprobar en los resultados, los p-valores devueltos para cada una de las comparaciones son altísimos, lo cual indica que estadísticamente no existe ningún algoritmo que destaque sobre los demás.

```{r}
# Test de Wilcoxon: QDA vs. KNN
qda.vs.knn <- wilcox.test(clasif_models$out_train_qda, clasif_models$out_train_knn)
# Test de Wilcoxon: LDA vs. KNN
lda.vs.knn <- wilcox.test(clasif_models$out_train_lda, clasif_models$out_train_knn)
# Comparativa de los grupos LDA vs. QDA | QDA vs. KNN | LDA vs. KNN con la corrección de Holm
p.adjust(c(lda.vs.qda$p.value, qda.vs.knn$p.value, lda.vs.knn$p.value), method = "holm")
```

